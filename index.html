<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Kristi's Blog</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="shortcut icon" href="https://KristiGourlay.github.io/blog/theme/favicon.ico" />
        <link rel="apple-touch-icon" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone4.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad3.png" />

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/colorbox.css">

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/main.css">

        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    </head>
    <body>

        <div class="container">
            <div class="row">
                <div class="span9">
                    <div class="row">
                        <div class="span2"> <!-- logo -->
                            <div id="logo">
                                <a href="https://KristiGourlay.github.io/blog">
                                    <img alt="Kristi's Blog" src="https://KristiGourlay.github.io/blog/" />
                                </a>
                            </div> <!-- logo -->
                        </div> <!-- logo span -->

                        <div class="span7"> <!-- header -->
                            <!--[if lt IE 9]>
                                <p class="chromeframe alert alert-warning">You are using an <strong>outdated</strong> browser, and this site might not look best in it. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
                            <![endif]-->
                            <header id="site-header">
                                <div id="site-header-content">
                                    <h1>
                                        <a href="https://KristiGourlay.github.io/blog">Kristi's Blog</a>
                                        <small>
                                            <span class="divider">/</span> 
                                        </small>
                                    </h1>
                                    <p>
                                        
                                    </p>
                                </div>
                            </header>
                        </div> <!-- header span -->
                    </div> <!-- header row -->

                    <div class="row"> <!-- content -->
                        <div class="content">
    <div id="index">
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_6.html" rel="bookmark">Pipelines and Transformers</a></h2>
            <time datetime="2019-07-08T12:10:00-04:00">
                Jul 08, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div id="first-article">
        <p>Since my Data Science Bootcamp ended a little over a month ago, I've spent most of my time going over what I had learned and refreshing my skills. Most of my time has been spent going deeper into areas that we covered briefly. For example, I have been practicing with Keras, SQL, and time series programs like Prophet. In the last few days, I decided to go deeper into something that I did use frequently in my projects, but not to the full extent of what it is designed for. This powerful resourse is called a pipeline and it's available (like most tools I have come to love) through the Scikit-learn libraries. Scikit-learn Pipeline is used to automate machine learning workflows. Pipelines create a linear sequence of data transforms to be chained together in a process that can be evaluated all in one step. In addition to Pipelines, I am also going to tackle something completely new to me which is ColumnTransformers. A Column Transformer can be paired with a Pipeline to simplify the workflow. Let's start by importing our essential libraries and reading in our data. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">joblib</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/brazil_cities.csv&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CITY</th>
      <th>STATE</th>
      <th>CAPITAL</th>
      <th>IBGE_RES_POP</th>
      <th>IBGE_RES_POP_BRAS</th>
      <th>IBGE_RES_POP_ESTR</th>
      <th>IBGE_DU</th>
      <th>IBGE_DU_URBAN</th>
      <th>IBGE_DU_RURAL</th>
      <th>IBGE_POP</th>
      <th>...</th>
      <th>Pu_Bank</th>
      <th>Pr_Assets</th>
      <th>Pu_Assets</th>
      <th>Cars</th>
      <th>Motorcycles</th>
      <th>Wheeled_tractor</th>
      <th>UBER</th>
      <th>MAC</th>
      <th>WAL-MART</th>
      <th>POST_OFFICES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>São Paulo</td>
      <td>SP</td>
      <td>1</td>
      <td>11253503.0</td>
      <td>11133776.0</td>
      <td>119727.0</td>
      <td>3576148.0</td>
      <td>3548433.0</td>
      <td>27715.0</td>
      <td>10463636.0</td>
      <td>...</td>
      <td>8.0</td>
      <td>1.947077e+13</td>
      <td>2.893261e+12</td>
      <td>5740995.0</td>
      <td>1134570.0</td>
      <td>3236.0</td>
      <td>1.0</td>
      <td>130.0</td>
      <td>7.0</td>
      <td>225.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Osasco</td>
      <td>SP</td>
      <td>0</td>
      <td>666740.0</td>
      <td>664447.0</td>
      <td>2293.0</td>
      <td>202009.0</td>
      <td>202009.0</td>
      <td>NaN</td>
      <td>616068.0</td>
      <td>...</td>
      <td>2.0</td>
      <td>6.732330e+12</td>
      <td>1.321699e+10</td>
      <td>283641.0</td>
      <td>73477.0</td>
      <td>174.0</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Rio De Janeiro</td>
      <td>RJ</td>
      <td>1</td>
      <td>6320446.0</td>
      <td>6264915.0</td>
      <td>55531.0</td>
      <td>2147235.0</td>
      <td>2147235.0</td>
      <td>NaN</td>
      <td>5426838.0</td>
      <td>...</td>
      <td>5.0</td>
      <td>2.283445e+12</td>
      <td>9.738864e+11</td>
      <td>2039930.0</td>
      <td>363486.0</td>
      <td>289.0</td>
      <td>1.0</td>
      <td>68.0</td>
      <td>1.0</td>
      <td>120.0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 81 columns</p>
</div>

<div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(5576, 81)
</pre></div>


<p>I've decided to use data on Brazilian cities. For the purpose of this blog, I am going to only look at 8 columns which relate to: State, Capital, Population, Life Expectancy, Education Index, GDP, Number of Companies, and Tourism Category.</p>
<div class="highlight"><pre><span></span><span class="n">keep</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;STATE&#39;</span><span class="p">,</span> <span class="s1">&#39;CAPITAL&#39;</span><span class="p">,</span> <span class="s1">&#39;IBGE_RES_POP&#39;</span><span class="p">,</span> <span class="s1">&#39;IDHM_Longevidade&#39;</span><span class="p">,</span> <span class="s1">&#39;IDHM_Educacao&#39;</span><span class="p">,</span> <span class="s1">&#39;GDP&#39;</span><span class="p">,</span> <span class="s1">&#39;COMP_TOT&#39;</span><span class="p">,</span> <span class="s1">&#39;CATEGORIA_TUR&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;STATE&#39;</span><span class="p">:</span> <span class="s1">&#39;state&#39;</span><span class="p">,</span> <span class="s1">&#39;CAPITAL&#39;</span><span class="p">:</span> <span class="s1">&#39;capital&#39;</span><span class="p">,</span> <span class="s1">&#39;IBGE_RES_POP&#39;</span><span class="p">:</span> <span class="s1">&#39;population&#39;</span><span class="p">,</span>
                <span class="s1">&#39;IDHM_Longevidade&#39;</span><span class="p">:</span> <span class="s1">&#39;life_expectancy&#39;</span><span class="p">,</span> <span class="s1">&#39;IDHM_Educacao&#39;</span><span class="p">:</span> <span class="s1">&#39;education_index&#39;</span><span class="p">,</span>
                <span class="s1">&#39;GDP&#39;</span><span class="p">:</span> <span class="s1">&#39;gdp&#39;</span><span class="p">,</span> <span class="s1">&#39;COMP_TOT&#39;</span><span class="p">:</span> <span class="s1">&#39;num_companies&#39;</span><span class="p">,</span>
                <span class="s1">&#39;CATEGORIA_TUR&#39;</span><span class="p">:</span> <span class="s1">&#39;tourism_category&#39;</span><span class="p">,})</span>
<span class="c1">#Renaming the columns with more descriptive names</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>


<div class="highlight"><pre><span></span>state                object
capital               int64
population          float64
life_expectancy     float64
education_index     float64
gdp                 float64
num_companies       float64
tourism_category     object
dtype: object
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>state                  0
capital                0
population             8
life_expectancy        8
education_index        8
gdp                    3
num_companies          3
tourism_category    2288
dtype: int64
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;population&#39;</span><span class="p">])</span> <span class="c1"># Drop all NaN in target column</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>capital</th>
      <th>population</th>
      <th>life_expectancy</th>
      <th>education_index</th>
      <th>gdp</th>
      <th>num_companies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>capital</th>
      <td>1.000000</td>
      <td>0.567143</td>
      <td>0.050703</td>
      <td>0.113596</td>
      <td>0.420889</td>
      <td>0.479255</td>
    </tr>
    <tr>
      <th>population</th>
      <td>0.567143</td>
      <td>1.000000</td>
      <td>0.081922</td>
      <td>0.137562</td>
      <td>0.942853</td>
      <td>0.960272</td>
    </tr>
    <tr>
      <th>life_expectancy</th>
      <td>0.050703</td>
      <td>0.081922</td>
      <td>1.000000</td>
      <td>0.704590</td>
      <td>0.071787</td>
      <td>0.091642</td>
    </tr>
    <tr>
      <th>education_index</th>
      <td>0.113596</td>
      <td>0.137562</td>
      <td>0.704590</td>
      <td>1.000000</td>
      <td>0.105869</td>
      <td>0.135122</td>
    </tr>
    <tr>
      <th>gdp</th>
      <td>0.420889</td>
      <td>0.942853</td>
      <td>0.071787</td>
      <td>0.105869</td>
      <td>1.000000</td>
      <td>0.946001</td>
    </tr>
    <tr>
      <th>num_companies</th>
      <td>0.479255</td>
      <td>0.960272</td>
      <td>0.091642</td>
      <td>0.135122</td>
      <td>0.946001</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Reds&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10ba1ccf8&gt;
</pre></div>


<p><img alt="png" src="images/blog_6_14_1.png"></p>
<h1>Train Test Split</h1>
<p>After some basic EDA, it's time to split the data into training and testing sets. The y (target) is going to be the population, and the rest of the columns will be the features I am using to predict the population of a city in Brazil.</p>
<div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;population&#39;</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;population&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>state</th>
      <th>capital</th>
      <th>life_expectancy</th>
      <th>education_index</th>
      <th>gdp</th>
      <th>num_companies</th>
      <th>tourism_category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SP</td>
      <td>1</td>
      <td>0.855</td>
      <td>0.725</td>
      <td>6.870359e+08</td>
      <td>530446.0</td>
      <td>A</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SP</td>
      <td>0</td>
      <td>0.840</td>
      <td>0.718</td>
      <td>7.440269e+07</td>
      <td>15315.0</td>
      <td>B</td>
    </tr>
    <tr>
      <th>2</th>
      <td>RJ</td>
      <td>1</td>
      <td>0.845</td>
      <td>0.719</td>
      <td>3.294314e+08</td>
      <td>190038.0</td>
      <td>A</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(4454, 7)
(1114, 7)
(4454,)
(1114,)
</pre></div>


<p>At this point in my work flow, I would import dataframe mapper from Scikit-learn and I would start to fix each column individually. However, this is where I am going to explore a new tool to me, Scikit-Learn's ColumnTransformer, which will simultaneously transform several columns, which need the same transformation process. </p>
<h1>Classify Columns into Categorical and Numerical</h1>
<p>Columns do not necessarily need to be separated into these two type of columns, but in this case it makes for a better example. This is a perfect example showing when you would used dataframe mapper and when you would use this process instead. If you have several columns that need very diferent types of transforming, I would used dataframe mapper, if you have a more homogenous group, where you have several columns that need to be transformed in a similar way, I would use this process. It makes for a cleaner workflow.</p>
<p>After performing basic EDA, it became apparent that the columns State, Capital, and Tourism Category, could be defined as Categorical, and they will need to be One Hot Encoded and since there are missing values, we will need to use an Imputer. The columns Life Expectancy, Education Index, GDP, and Number of Companies, could be defined as Numerical, and they will need to be scaled, as well as have the missing values imputed.</p>
<div class="highlight"><pre><span></span><span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">,</span> <span class="s1">&#39;capital&#39;</span><span class="p">,</span> <span class="s1">&#39;tourism_category&#39;</span><span class="p">]</span>
<span class="n">num_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;life_expectancy&#39;</span><span class="p">,</span> <span class="s1">&#39;education_index&#39;</span><span class="p">,</span> <span class="s1">&#39;gdp&#39;</span><span class="p">,</span> <span class="s1">&#39;num_companies&#39;</span><span class="p">]</span>
</pre></div>


<h1>Build a Pipeline for Categorical Columns</h1>
<p>Now I am going to start building pipelines for these two types of columns. Let's start with our Categorical columns.</p>
<div class="highlight"><pre><span></span><span class="n">cat_cols</span> <span class="c1">#remind us what our categorical columns are</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;state&#39;, &#39;capital&#39;, &#39;tourism_category&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_cat</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">cat_cols</span><span class="p">]</span> <span class="c1">#pulling out the cat columns from our X_train</span>
<span class="n">X_train_cat</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>state</th>
      <th>capital</th>
      <th>tourism_category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>200</th>
      <td>SP</td>
      <td>0</td>
      <td>C</td>
    </tr>
    <tr>
      <th>3517</th>
      <td>SP</td>
      <td>0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1907</th>
      <td>PR</td>
      <td>0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">X_train_cat</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> 
</pre></div>


<div class="highlight"><pre><span></span>state                  0
capital                0
tourism_category    1836
dtype: int64
</pre></div>


<p>After seeing that there are 1815 missing values, and we decide that we dont want to lose any of the valuable information in the rest of the values, we decide to impute those missing values. </p>
<div class="highlight"><pre><span></span><span class="n">si</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="s1">&#39;other&#39;</span><span class="p">)</span> <span class="c1">#This will fill all missing values as &#39;other&#39;</span>

<span class="n">si</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_cat</span><span class="p">)</span>
<span class="n">X_train_cat_si</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train_cat</span><span class="p">)</span>
</pre></div>


<p>After imputing the missing values, we will now need to one hot encode the columns.</p>
<div class="highlight"><pre><span></span><span class="n">ohe</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">X_train_cat_ohe</span> <span class="o">=</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_cat_si</span><span class="p">)</span>
</pre></div>


<p>Now that we have imputed all missing values and one hot encoded the categorical columns, we will now place these two steps into a pipeline. We do this by defining them as steps and then placing them in the pipeline. The steps are an array of tuples that contain two values. The first value in a step is the name you are giving the process and the second value is the instantiated step (example si is SimpleImputer and ohe is OneHotEncoder). These steps are then placed in the pipeline and the pipe fits both the X_train and y_train.</p>
<div class="highlight"><pre><span></span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;impute&#39;</span><span class="p">,</span> <span class="n">si</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;ohe&#39;</span><span class="p">,</span> <span class="n">ohe</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">cat_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">cat_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_cat</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Pipeline(memory=None,
     steps=[(&#39;impute&#39;, SimpleImputer(copy=True, fill_value=&#39;other&#39;, missing_values=nan,
       strategy=&#39;constant&#39;, verbose=0)), (&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None,
       dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;error&#39;,
       n_values=None, sparse=False))])
</pre></div>


<h1>Build a Pipeline for Numerical Columns</h1>
<div class="highlight"><pre><span></span><span class="n">num_cols</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;life_expectancy&#39;, &#39;education_index&#39;, &#39;gdp&#39;, &#39;num_companies&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_num</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">num_cols</span><span class="p">]</span> <span class="c1">#pulling out the numerical columns from our X_train</span>
<span class="n">X_train_num</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>life_expectancy</th>
      <th>education_index</th>
      <th>gdp</th>
      <th>num_companies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>200</th>
      <td>0.863</td>
      <td>0.687</td>
      <td>2530729.24</td>
      <td>2464.0</td>
    </tr>
    <tr>
      <th>3517</th>
      <td>0.855</td>
      <td>0.616</td>
      <td>177756.32</td>
      <td>213.0</td>
    </tr>
    <tr>
      <th>1907</th>
      <td>0.843</td>
      <td>0.676</td>
      <td>351801.06</td>
      <td>731.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>The numerical columns need to be scaled and imputed. We could transform the columns by Standard Scaling and then transform the columns by SimpleImputing, in a two step process, like we did for the Categorical Columns, but we can actually just skip those two individual processes and place the two steps directly into the pipeline. We simply instantiate the two processes we need to complete, and then skip right to the last step, defining them within the steps and placing those steps into the pipeline.</p>
<div class="highlight"><pre><span></span><span class="n">si</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;impute&#39;</span><span class="p">,</span> <span class="n">si</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;ss&#39;</span><span class="p">,</span> <span class="n">ss</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">num_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">num_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_num</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Pipeline(memory=None,
     steps=[(&#39;impute&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;mean&#39;,
       verbose=0)), (&#39;ss&#39;, StandardScaler(copy=True, with_mean=True, with_std=True))])
</pre></div>


<h1>Use ColumnTransformer to Concatenate all Data Together</h1>
<p>The ColumnTransformer can now take both pipelines, concatenating them together, to transform the entirety of the X_train. </p>
<p>A ColumnTransformer takes an array of tuples with three values. The first value is the name we give the process, the second value is the pipeline created, and the third value is the list of columns that the pipeline was created to transform.</p>
<div class="highlight"><pre><span></span><span class="n">transformers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_pipe</span><span class="p">,</span> <span class="n">cat_cols</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">num_pipe</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">ct</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="n">transformers</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_trans</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_trans</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(4454, 39)
</pre></div>


<h1>Create a Final Pipeline with a Machine Learning Model</h1>
<p>Now we can take the transformer and a machine learning model, such as Linear Regression, and create a final pipeline that takes in all the raw data, transforms the data through the individual pipelines, concatenates the data with the transformer, and then predicts.</p>
<div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">final_steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="n">ct</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">final_steps</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Pipeline(memory=None,
     steps=[(&#39;transformer&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3,
         transformer_weights=None,
         transformers=[(&#39;cat&#39;, Pipeline(memory=None,
     steps=[(&#39;impute&#39;, SimpleImputer(copy=True, fill_value=&#39;other&#39;, missing_values=nan,
       strategy=&#39;constant&#39;, ve...(&#39;model&#39;, LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
         normalize=False))])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.9648703985990568
</pre></div>


<p>Now let's see if the pipeline will work on the test data. The test data was placed aside when we did our train test split. It has not been used for any of the processes above.</p>
<div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">y_pred</span>
</pre></div>


<div class="highlight"><pre><span></span>array([ 2080.,  5952., 11360., ...,  5632., 30720., 15680.])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.7800829672957542
</pre></div>


<h1>Concluding Remarks</h1>
<p>There are many benefits to using a Scikit-learn pipeline. By enforcing and implementing the order of steps, it makes a workflow much easier to read and understand. Pipelines are especially useful, if you are working with datasets that are continuously being added to. A pipeline can be used to transform the new data in the same process. </p>
<p>While I am a big fan of DataFrameMapper, which can also be placed into a pipeline, I often found myself using DataFrameMapper and having to repeat myself, as many columns need the exact same transformations. With the process above, it makes it much simpler, easier, and less time consuming. While I find this new process advantageous, I need to note that there are many advantages for using DataFrameMapper. The main one that comes to mind for me is that DataFrameMapper allows you to keep the annotations and labels that you assigned to your pandas dataframe. The results of my pipeline above, on the other hand, will reduce all information to a array/matrix. This makes DataFrameMapper much easier to comprehend. Both processes have their benefits, and I envision using both in different scenarios.  </p>
<p>Finally, pipelines can also then be used to GridSearch and find the best models and hyper-paramters. As we can see from our train and test scores, our current model is quite overfit, a GridSearch could aid in finding a model that is less overfit, and our test score could be improved.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_5.html" rel="bookmark">OxyBeerOn: A Beer Recommender App</a></h2>
            <time datetime="2019-06-04T10:20:00-04:00">
                Jun 04, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>Last week, I finished my Data Science boot camp at Bitmaker General Assembly in Toronto. For the final two weeks, I spent my time working on my final project, known as the Capstone Project. This blog will be an overview of this project, from its inception to my presentation last Thursday. For the project, I decided to make a beer recommender system. There were various reasons as to why I chose this project. First, I've been bartending for the last three years, and I've become fairly knowledgeable about beer, and have become a human recommender system in the process. Most customers are not very well-informed about the different styles of beers, and a recommender system could help people to figure out which beers they would most prefer. Second, we only spent a couple hours during the 12-week course on recommender systems, so I thought it would be a challenge to dig deeper into the area. Third, it became quite obvious when I was researching the various project ideas I had, that it was almost impossible to find examples of recommender systems online, that were not based on the MovieLens dataset. This sealed the deal for me, knowing that I would have to wrangle my own data and model without the benefit of a bevy of online examples. </p>
<h1>Step One: Web Scraping</h1>
<p>The first thing I needed to do was find my beer data. The first place I found was the website Brewery DB. Brewery DB is an online database, developed for brewers and web developers to power their apps. After purchasing an API key, I spent 4 days requesting beers from the website. The first obstacle I came accross was that unless I purchased the most expensive API key, I did not have the ability refine my scraping to breweries located in Ontario. So although I scraped 20,000+ beers from the site, after filtering down to beers from Ontario and Quebec, Macro-Breweries, and well-known breweries in the US, with beers readily available in Toronto, I was only left with about 530 beers. Because of this, I decided to supplement my data, by scraping the Top Rated 100 beers in Ontario list from the Beer Advocate website. </p>
<p><img src='images/beer_advocate.png' alt='beer_adv'></p>
<p>After scraping these two sites, and concatenating the information I wrangled, I was left with 629 beers. The next thing I needed to do was create a 'user base'. To create this database of users, I relied on two methods. The first was to create and disseminate online, a basic beer form, comprised of 20 beers, both representative of styles, and micro and macro breweries. </p>
<p><img src='images/beer_form.png' alt='beer_form'> <img src='images/beer_form2.png' alt='beer_form2'></p>
<p>Users were asked to rate the 20 beers on a scale from 1 to 10. In addition to this simple form, I was able to convince 14 of my friends (myself being number 15) to complete the entire list of 629 beers.</p>
<p><img src='images/user_spreadsheet.png' alt='users'></p>
<p>The final user/beer database had:</p>
<div class="highlight"><pre><span></span>        -86 Beer Forms filled out
        -15 &quot;Super-Users&#39;
        -338 Beers (after I deleted beers with no ratings)
</pre></div>


<p>The hope was that while 86 people only were able to rate 20 beers, the super users would bridge this sparsity, and similarities would be made between regular users and super-users, so that beers that the regular users had not tried could be recommended to them, based on super-user ratings.</p>
<h1>Step Two: Modeling</h1>
<p><img src='images/user_colaborative.png' alt='models'></p>
<p>There are three types of Recommender Systems:</p>
<p>Content Based. Broadly speaking, this system is based on the premise that if you like one item, you will like another similar item. In terms of beer, if you like Guinness, you will most likely like other stouts, especially ones poured with a creamer.</p>
<p>Collaborative Based. This system focuses on the relationship between users and items. Strictly speaking, similar people like similar items. If person A and B, like Corona, Coors Light, and Heineken, and person A loves Sawdust City Little Norway, a Collaborative Based recommender system would recommend Little Norway to person B.</p>
<p>Hybrid Systems. This system combines Content and Collaborative systems. For example, if person C likes imperial stouts and IPAs. And person D also likes imperial stouts, but hates IPAs, a hybrid system would recommend other imperial stouts that person C liked to person D.</p>
<p><img src='images/spotlight.png' alt='spotlight'></p>
<p>Model 1: Spotlight Recommender Model (Collaborative)</p>
<p>For the first model I used the Spotlight Recommender system. Spotlight was developed by Maciej Kula, and is the recommendation system used by Netflix. Spotlight provides frameworks for Sequential models, as well as, Factorization models, both Implicit and Explicit. Due to the small dataset, the Spotlight model was not able to make accurate recommendations for users. The precision at K was .18 after tuning parameters. The recommendations seemed to be skewed by the data compiling decisions I had made. For none super users, it would only recommend those 20 beers thats were part of the Beer Form.</p>
<p><img src='images/spotlightcode.png' alt='spotlightcode'></p>
<p>Model 2: KNN Model (Collaborative)</p>
<p>A KNN model is a non-parametric model that seperates data points into several clusters and then makes inferences for new samples by measuring the distance between the target beer and the closest beers to it. This model relies on user-item ratings.</p>
<p><img src='images/knn.png' alt='knn'></p>
<p>While a regular KNN model will try to classify a new data point by measuring the distance between the new point and all other points, and returning the K nearest neighbours. In a recommender system, the model is not trying to classify the new point, but is trying to find items with similar user-item interactions. The KNN model seemed to work quite well. The function below takes in a beer name, and will recommend 10 beers with similar user-item ratings. </p>
<p><img src='images/knn_code.png' alt='knn'></p>
<p>Using Bellwood's Double Justu as an example, the KNN model, returned the following 10 beers as recommendations (shown below). This is rather impressive when taking into account that this model does not take styles into consideration. In addition to its regular IPA version, Jutsu, it also returned Bellwood's Witchshark, Farmageddon, and Roman Candle, three of their higher percentage beers. </p>
<p><img src='images/find_similar_beer_function.png' alt='fsbf'></p>
<p>Model 3: Cosine Similarity Model (Content)</p>
<p><img src='images/cosine.png' alt='cosine'></p>
<p>Cosine Similarity measures the similarity between two non-zero vectors. For example, two vectors that share the same orientation have a cosine similarity of 1. This method recommended beer based on similar styles. Due to the relatively small dataset, I personally mapped over 100s styles down to 31.</p>
<p><img src='images/cosine_code.png' alt='cc'></p>
<p>This model, in theory, is very simplistic. It will return beers that are similar in styles. For example, if you ask for similar beers to Heineken, the function will return these beers:</p>
<p><img src='images/return_beer_function.png' alt='rbf'></p>
<p>Model 4: Kristi's Hybrid Model (oxybeeron.py)</p>
<p>My hybrid model (which is connected to the flask app), provides 10 beer recommendations based on the collaboration of the above three models. All three models above returned beer recommendations, to varying degrees of accuracy. Because of the small dataset, the Cosine Similarity model was most accurate. Thus, for the flask app, I decided to make this model more heavily weighted. However, anything that was recommended by Model 1 (Spotlight) and Model 2 (KNN), the two models based on the colaborative method, were automatically recommended, and then the final recommendation spots were filled out by Model 3 (Cosine Similarity). The flask app, allows the 'online' user to choose from a list of 10 beers; their preferences, and then this model, in turn, recommends beer based on this hybrid model.</p>
<p>The following are two snippets of the code used to create the final recommendations. To see the entire code used for the project, please visit <a href='https://github.com/KristiGourlay/capstone_project'>My GitHub</a></p>
<p><img src='images/function1.png' alt='function1'></p>
<p><img src='images/function2.png' alt='function2'></p>
<h1>Step Three: Flask App</h1>
<p>The final requirement of the project was to serve the app locally, and try to have the app more interesting that just a white page with input boxes. </p>
<p>I decided to create four different templates, so that if the user did not know any of the beer options, then they could refresh the page and get new options. As you can see below, the final flask app, takes 'inputs' from the site, and the two functions shown above, provide 10 recommended beers based on the input.</p>
<p><img src='images/flask_code.png' alt='flask_code'></p>
<p>The final app looked like this:</p>
<p><img src='images/flask_app_page.png' alt='flask_app_page'></p>
<p><img src='images/flask_result_page.png' alt='results'></p>
<h1>Step Four: Taking it further!</h1>
<p>The more I developed this project, the more I came to realise that the sparseness of the data hindered what I could do with my modeling. While my final hybrid model recommended accurate beers using the three models (Spotlight, KNN, and Cosine Similarity), I began to realise that these three models could also be used in tandem for different configurations of data. What started as an attempt to recommend beer for my 'super-users' as a thank you for their participation, turned into another Super-User Model (also located on <a href='https://github.com/KristiGourlay/capstone_project'>My GitHub</a>). This hybrid model is an example of how the collaboration of these three models can be manipulated to conform to different datasets. This model does not work on the entirety of the user base, but for those who filled out the entire Google Spreadsheet (my 'Super Users), this model is effective. This model relies on the top recommendations provided by Spotlight. It then fetches all the beer that a certain user rated as 8 or above and then uses the cosine similarity and KNN model functions to list content similarity and user-item interaction similarity to those specific beers. The final function takes in a user id number and then recommends the beers that are present in the Spotlight recommendations, and in the combination of the KNN and cosine similarity recommendations. This model effectively recommends beer based on similar user preferences, but also considers the users preferences for style.</p>
<p><img src='images/super_function1.png' alt='sf'></p>
<p><img src='images/super_function2.png' alt='sf2'></p>
<h1>Final Thoughts</h1>
<p>The oxybeeron app was a nice introduction to recommender systems. It relied more heavily on, and was more effective with, the more basic models, like KNN and Cosine Similarity. This was simply because there was not enough data for Spotlight to do its magic. However, the combination of these three models, provided a basic blueprint for a hybrid model that could be quite effective.</p>
<p>The effectiveness of the Cosine Similarity Model with only 31 beer styles, makes me optimistic about its potential in a larger dataset where the number of beer styles could be more expansive. For example, instead of "IPAs", it could include specific IPAs (Double IPAs, Session IPAs, Vermont-Style IPAs, Black IPAs, hazy IPAs, etc.)</p>
<p>Overall, it became painfully obvious that it's hard to find information and examples online for recommender systems that are not based on the MovieLens dataset. I think that for Spotlight to work to the best of its ability, my dataset would need to be much larger than 101 users and 338 beers. However, the improvement in its ability to recommend beer for my super users is promising. My Super-User Model shows that in the interim between a cold start situation and a 100K+ dataset (aka MovieLens), Spotlight can be quite effective if it is combined with other recommendation algorithms. Using these three models, in tandem, resulted in the ability to make solid recommendations for users in a smaller network.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_4.html" rel="bookmark">You Pro-BAYE-bly will FREQUENT this Post Before an Interview</a></h2>
            <time datetime="2019-05-08T10:20:00-04:00">
                May 08, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>Most Data Science questions deal with studying populations. A population is a set of similar items or events that are of interest for a question or an experiment. Since the task of measuring  an entire population is frequently too expensive and impractical, we take samples, and make inferences about the whole population based on the statistics we find in the sample. In statistical inference we have four key concepts:</p>
<div class="highlight"><pre><span></span>            -Samples
            -Statistics
            -Parameters
            -Populations
</pre></div>


<p>Generally speaking, statistics describe samples and parameters describe populations. Statistical inference is how we move from statistics taken on a sample, to parameters about the whole population. For Frequentists the two main ways we can generalize from a sample to a populations are through Confidence Intervals and Hypothesis Tests. For Bayesians, probability is assigned to a hypothesis, where a prior probability is updated  to a posterior probability, with relevant data. This blog post will outline and compare these two theories in light of Data Science.</p>
<h1>Frequentist Probability</h1>
<p>Frequentists have three mathematical concepts that their experiments rely on - the null hypothesis, the associated p value, and the confidence interval. Frequentists have a null hypothesis (known as H0) and an alternate hypothesis (H1). An example of this is if a scientist was running a drug trial where he split the subjects into two groups: those administered the drug and those administered a placebo. The null hypothesis would be the assumption that there was no difference between the two groups. And this would be the stance of the scientist. After this, the scientist would create a reasonable threshold for rejecting the null hypothesis. This notation - α (alpha) - a real number between 0 and 1 - is known as the p value. The p value is the probability of observing a deviation from the null hypothesis which is greater than or equal to the one you observed. Another way of thinking about it, is that the p value is the probability of the data, given that the null hypothesis is true. The scientist will reject the null hypothesis if the p value is below α and not reject otherwise. Alpha is generally set to 0.05. This is considered standard practice.</p>
<p>The best way to explain p value and the null hypothesis is by a drug efficacy example. Let's say that we've just created a new sleeping pill. We've randomly selected 100 people who have problems sleeping; 50 will be administered the sleeping pill, and 50 will be given a placebo. The group given the sleeping pill is the "experiment" group and the group given the placebo is known as the "control" group. In this situation the null hypothesis will be that there will be no difference in the sleeping patterns - hours slept - between the two groups. The alternate hypothesis is that there will be a difference in hours slept between the two groups. The p value (the level of significance) will be .05. Thus, we will reject the null hypothesis if the p value is below .05.</p>
<p>Let's simulate two groups - control and experiment - and their average hours slept per night during the trial.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">control</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">control</span><span class="p">)</span>
<span class="n">experiment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">experiment</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[8 5 7 7 4 5 6 2 6 3 3 8 6 6 7 5 8 3 6 7 4 2 8 3 3 4 9 4 5 3 8 8 5 2 9 3 8
 8 9 2 9 8 7 9 2 3 8 9 8 5]
[6 4 9 9 6 9 8 5 8 4 7 9 7 9 9 5 5 5 4 6 7 6 9 7 6 8 8 5 8 4 4 8 5 7 4 9 5
 5 9 6 5 7 4 8 8 4 9 5 5 7]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">control</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">experiment</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">experiment</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">control</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>5.74
6.52
0.7799999999999994
</pre></div>


<p>The measure of difference is 0.78. What is the probability that we observed this data, assuming that the null hypothesis is true?</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">stats</span>

<span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">control</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Ttest_indResult(statistic=1.8573147670624264, pvalue=0.06626937134706777)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">control</span><span class="p">)</span>
</pre></div>


<p>The p value is .066, thus we must accept the null hypothesis and state that there is no difference between the sleeping patterns - hours slept - between the experiment and control groups.</p>
<p>The third important key in Frequentist statisitics is the confidence interval. Simply put, a confidence interval is a range of values, in which the actual value is likely to fall. It represents the accuracy or precision of a given estimate. A confidence interval can be calculated as so: </p>
<p><img src='images/calculation1.png' alt='calcu1ation1'></p>
<p>Let's simulate a population and a poll as an example. Say we live in a state and we are working for a newspaper and we are trying to determine whether a certain proposition will pass. Unfortunately, we dont have the time, or the money, to run a poll that will reach out and question all 1 million voters. If we were to run an experiment, we would poll a sample of the society, and then use those samples to make inferences about the whole population. This is how a frequentist would tackle this situation. First I will generate statistics about the state.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span> <span class="mf">0.60</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">1</span><span class="n">_000_000</span><span class="p">)</span> <span class="c1">#similating a population where 60% would vote yes.</span>
<span class="n">population</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([1, 0, 0, 1, 1, 1, 1, 0, 0, 0])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.554
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.614
</pre></div>


<p>In this scenario, the population is 1,000,000, the sample is 500 people, the statistics would be the percentage of people in the sample voting yes on the proposition, and the parameter is the true percentage of people voting yes on the proposition.</p>
<p>We at the newspaper, do not know that 60% of the population would vote for this proposition to pass, but by looking at a sample of 500 voters, we can make educated inferences about how the entire population may vote. This is the Central Limit Theorem in action. This is a probability theory that states that when independent random variables are added their sum tends toward a normal distribution, even if the original variables are not normally distributed. It is this theory that allows for conclusions to be made about the whole population from a sample of the population. Of the sample of the first 500 citizens who were polled, 55.4% responded that they would vote for the proposition to pass. A frequentist would then calculate the margin or error, creating the confidence interval by calculating the mean and the standard deviation and combining this with the associated Z score (1.96). The area under the standard normal distribution between -1.96 and +1.96 is 95%. The confidence interval is a set of likely values for the parameter of interest. </p>
<p><img src='images/196.jpg' alt='196'></p>
<div class="highlight"><pre><span></span><span class="n">sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">lower</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">sample_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">higher</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">sample_mean</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">f</span><span class="s1">&#39;I am 95</span><span class="si">% c</span><span class="s1">onfident that the true population percentage of people who will vote yes on this proposition is between {lower} and {higher}&#39;</span>
</pre></div>


<div class="highlight"><pre><span></span>&#39;I am 95% confident that the true population percentage of people who will vote yes on this proposition is between 0.5104 and 0.5976&#39;
</pre></div>


<p>'I am 95% confident that the true population percentage of people who will vote yes on this proposition is between 0.5104 and 0.5976'</p>
<p>Frequentist statistics allow us to make inferences about the greater population by using a smaller sample of the population and the statistics observed from that sample. The main critiques of Frequentist theory however, is that the p value and the confidence interval depend on the sample size, as Frequentist theory does not perform well on sparse data sets. Moreover, the confidence intervals are not probability distributions. These two flaws in Frequentist theory are remedied in Bayesian Statistics. </p>
<h1>BAYESIAN PROBABILITY</h1>
<p>Bayesian statistics successfully apply probabilities to statistical problems. In addition to this, it provides the ability to upgrade probabilities with the introduction of new data. It includes three components:</p>
<div class="highlight"><pre><span></span>            -The Prior
            -The Likelihood
            -The Posterior
</pre></div>


<p>The Prior is our belief about paramenters based on previous experience. This takes into account both previous data compiled, in addition to, our own beliefs based on experience.
The Likelihood is the specific observed data.
The Posterior distribution combines the prior and the likelihood. It is the multiplication of the Likelihood and the Prior.
The Posterior distribution is calculated by the following:</p>
<p><img src='images/calculation.png' alt='calculation'></p>
<p>For the sake of comparison with Frequentist theory, you could say that the Bayes factor is the equivalent of the p value. The Bayes factor is the ratio of the probability of one hypothesis in relation to the probability of another hypothesis. So for instance, in our sleeping pill example, the ratio of the probability of improved sleeping in the control group vs. that of the experiment group.</p>
<p>The High Density Interval (also known as the Credibility Interval) is the Bayesian equivalent to the Confidence Interval. The Credibility Interval is formed by the posterior distribution. While the Confidence Interval is a collection of intervals with 95% of them containing the true parameter, the Credibility Interval provides an interval that has a 95% chance of containing the true parameter. The Credibility Interval is independent of intentions and sample size. This is good for Data Science, because if we have few data points, we can use strong priors.</p>
<p>Now let's look at the same polling example from above performed with Bayesian inference. First we will create our prior. In this case, let's pretend that based on other statistics we've seen, and previous voting habits in this state, that we have a very strong prior belief that 20% of the state will vote yes on the proposition. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>


<span class="n">alpha_prior</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="mi">800</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">alpha_prior</span><span class="p">,</span> <span class="n">beta_prior</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prior Belief&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Values of P&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>


<p><img alt="png" src="images/blog_4_27_0.png"></p>
<p>Now we will incorporate our Likelihood and calculate the Posterior based on the Prior and the Likelihood. The Likelihood, calculated by n_trials and n_successes, will be equal to the data that we "collected" above in the Frequentist example.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">800</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">n_successes</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1">#copying the data from the sample used above</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">n_successes</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">n_successes</span> <span class="o">/</span> <span class="n">n_trials</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">n_successes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)],</span>
               <span class="n">ymin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
               <span class="n">ymax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">prior</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">)),</span> 
               <span class="n">linestyles</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
               <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior, Likelihood, and Posterior Modes&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Values of P&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>   
</pre></div>


<p><img alt="png" src="images/blog_4_29_0.png"></p>
<p>As you can see in the graph above, we have set our priors (a= 200, b= 800) very strong, so that even though the actual data collected was at around 60% voting yes on the proposition, the strength of our priors, has pulled the posterior mean to around 35%. 
If our prior beliefs were not strong, then we would set our alpha and beta much lower. In the following model, I will set them at 2 and 8 to show the effect.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">n_successes</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1">#copying the data from the sample used above</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">n_successes</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">n_successes</span> <span class="o">/</span> <span class="n">n_trials</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">n_successes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)],</span>
               <span class="n">ymin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
               <span class="n">ymax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">prior</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">)),</span> 
               <span class="n">linestyles</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
               <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior, Likelihood, and Posterior Modes&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Values of P&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>   
</pre></div>


<p><img alt="png" src="images/blog_4_31_0.png"></p>
<p>As you can see, with a weaker prior, the posterior is pulled to the left toward the prior, but only slightly. The posterior is being dominated by the data. A perfect example of when you would set a weak prior would be the first example above of a sleeping pill clinical trial. This makes the point that in Bayesian statistics, a prior is not necessary to draw a valuable conclusion (Although, we could specify a weak prior based on expert analysis in the field). In the sleeping pill example, the p value was 0.6, and thus this was interpreted as being insufficient evidence that the sleeping pill worked. There was clearly a difference observed between the two groups, but it was not judged to be sufficient enough of a difference in the standard Frequentist approach. Instead of testing whether two groups are different, Bayesians attempt to measure the difference between the two groups. A topic for a later blog!!!</p>
<p>CONCLUSION</p>
<p>While Frequentists would say that Bayesians incorporating prior beliefs is wrong, the basic idea of not including your prior beliefs, is already making a judgment call about the world. Including priors, results in more accurate distributions, because you're not including the probabilities for data points, that you know from common sense, are not possible. Furthermore, including priors allows us to incorporate expert opinions and specific knowledge into our models. When our model makes a prediction, it provides distribution of likely answers. These predictions are highly interpretable and easier to understand than p values and Confidence Intervals. Finally, if we do not have strong prior beliefs about certain areas, we can always set our prior beleifs to be weak. Thus, our posterior will be dominated by our data, and not our prior.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_3.html" rel="bookmark">Kitty Got Claws! A Comparison of CatBoost and XGBoost</a></h2>
            <time datetime="2019-04-23T10:20:00-04:00">
                Apr 23, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>Boosting is an ensemble technique where new models are created to correct the errors of past models. Each subsequent tree learns from the errors of its predecessor, as all trees are added sequentially until no further improvements can be made. Gradient boosting is an approach where new models are created from the residuals or errors of prior models and then are added together to make the final prediction. The base learners in gradient boosting are weak learners, but each contributes vital information for the final model. Two examples of these programs are XGBoost and CatBoost. They are two open-source software libraries that provide a gradient boosting framework. </p>
<p>To compare these two programs, I will load a dataset called "Skyserver", and I will attempt to predict whether an image is a Star, Galaxy, or Quasar. I will begin with XGBoost.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span><span class="p">,</span> <span class="n">FunctionTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">import</span> <span class="nn">catboost</span> <span class="kn">as</span> <span class="nn">cb</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">Pool</span>
<span class="kn">import</span> <span class="nn">shap</span>

<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_tree</span><span class="p">,</span> <span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">plot_importance</span>


<span class="n">sky</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Skyserver.csv&#39;</span><span class="p">)</span>
</pre></div>


<p>The first thing I do with a dataset is some basic EDA. This includes checking to see whether there are null values: sky.isnull().sum(). The Skyserver dataset is very clean and there are no null values. However, with XGBoost, even if there were null values, XGBoost has a built in algorithm to impute these missing values. Its algorithm automatically learns what is the best imputation value for missing values based on training. This is one key advantage of using XGBoost. </p>
<p>Before I am ready to model, I will prepare my dataset, by labeling the 3 items in the target column - 'star', 'galaxy', 'qso'. Then I will drop unnecessary columns, and assign my y(target) and X(features).</p>
<div class="highlight"><pre><span></span><span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;STAR&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;GALAXY&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;QSO&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sky</span> <span class="o">=</span> <span class="n">sky</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;objid&#39;</span><span class="p">,</span> <span class="s1">&#39;rerun&#39;</span><span class="p">,</span> <span class="s1">&#39;mjd&#39;</span><span class="p">])</span> 
<span class="c1">#drop &#39;objid&#39; because irrelevant to model and &#39;rerun&#39; because it has the same value for each row.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sky</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data_dmatrix</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">)</span> 
<span class="c1">#DMatrix is an internal data structure used in XGBoost. It is recommended as it is optimal for memory and speed.</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, &#39;base&#39;, None) is not None and \
/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version
  data.base is not None and isinstance(data, np.ndarray) \
</pre></div>


<p>Having completed basic EDA, assigned our X and y, and created a data dmatrix, we are ready to split out data into training and testing sets and use DataFrameMapper, StandardScaler and LabelBinarizer to prepare our data. We do not need to worry about imputing at this point, because as stated above, in addition to the dataset being clean of missing values, XGBoost has a built in imputer that deals with missing values.</p>
<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<p>Obviously a key difference between XGBoost and CatBoost is how each deals with categorical columns. I'll discuss CatBoost later, but for now, as is shown below, for XGBoost, categorical columns need to be one hot encoded, using LabelBinarizer. Knowing that creating dummy columns, in many instances, could expand datasets to unimaginable sizes, the XGBoost developers introduced a built-in method to deal with this, called Automatic Sparse Data Optimization. The developers chose to handle this in the same way that they handled the existence of missing values, by making the algorithm aware of the sparsity pattern in the dataset. By only visiting the present values, and not the missing values and zeroes; it exploits the sparsity and learns the best directions to handle missing values. It handles all cases of sparsity in a unified way. </p>
<div class="highlight"><pre><span></span><span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
    <span class="p">([</span><span class="s1">&#39;ra&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;dec&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;u&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;run&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;camcol&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;specobjid&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;redshift&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;plate&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;fiberid&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
<span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df_out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Z_train</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_val</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">DataConversionWarning</span><span class="p">)</span>
</pre></div>


<p>Now it's time to model, predict, and score!</p>
<div class="highlight"><pre><span></span><span class="n">model1</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="c1">#How many trees.</span>
    <span class="n">num_class</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="c1">#Identifying there are 3 classes in our target</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="s1">&#39;reg:logistic&#39;</span><span class="p">,</span> <span class="c1">#Objective refers to which model you want to use.</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="c1">#Eval_set refers to how you want your model to be evaluated.</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1">#Learning_rate has to do with how much you want each tree to learn.</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1">#stop after ten iterations if no improvements have been made.            </span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1">#Smaller trees are more interpretable</span>
    <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1">#Silent deals with whether we want information for each iteration to be printed out.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
       colsample_bytree=1, early_stopping_rounds=10, eval_set=&#39;Accuracy&#39;,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
       min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,
       nthread=None, num_class=3, objective=&#39;multi:softprob&#39;,
       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
       seed=None, silent=False, subsample=1)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z_val</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.01
</pre></div>


<p>Another key feature is that there is a built-in cross validation. This runs a cross validation at each iteration of the boosting process.</p>
<div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span> <span class="s1">&#39;0.3&#39;</span><span class="p">,</span> <span class="c1">#the fraction of columns to be randomly sampled for each tree.</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="s1">&#39;0.2&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="c1">#the maximum depth of a tree</span>
        <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="s1">&#39;10&#39;</span><span class="p">}</span> <span class="c1">#L1 regularization weight - Lasso</span>

<span class="n">cross_val_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">data_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span> <span class="c1">#this is where the dmatrix comes in handy.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">cross_val_results</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train-rmse-mean</th>
      <th>train-rmse-std</th>
      <th>test-rmse-mean</th>
      <th>test-rmse-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.615842</td>
      <td>0.001887</td>
      <td>0.617878</td>
      <td>0.001618</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.514400</td>
      <td>0.023372</td>
      <td>0.517281</td>
      <td>0.024252</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.456278</td>
      <td>0.013461</td>
      <td>0.460037</td>
      <td>0.015424</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.420923</td>
      <td>0.006550</td>
      <td>0.426109</td>
      <td>0.009830</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.385381</td>
      <td>0.030245</td>
      <td>0.391525</td>
      <td>0.033427</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">((</span><span class="n">cross_val_results</span><span class="p">[</span><span class="s2">&quot;test-rmse-mean&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>9    0.281167
Name: test-rmse-mean, dtype: float64
</pre></div>


<div class="highlight"><pre><span></span><span class="n">xgreg</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="o">=</span><span class="n">data_dmatrix</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>


<p>A benefit of using boosting is that it provides you with feature importance. It provides you with a score which indicates how valuable a feature was in the construction of the boosted decision tree. The more a certain feature is used to make key decisions in the decision tree, the higher its importance. Importance is calculated by the the amount that each split point improves the overall tree, weighted over the number of observations it is resposible for.</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="n">plot_importance</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/blog_3_23_0.png"></p>
<p>As can be viewed by the chart, Redshift is by the far the most important feature.</p>
<p>XGBoost also provides the ability to peak into your model and inspect a tree. </p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">xgreg</span><span class="p">,</span> <span class="n">num_trees</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/blog_3_25_0.png"></p>
<p>Above you can see how a decision tree on this data looks. You can see that it is assymetrical, which is an important point of comparison with CatBoost's trees. One interesting aspect about this tree is the fact that you can see that the algorithm has chosen to impute all 'missing' values with the direction of yes. </p>
<p>According to XGBoost's documentation, website, and papers given by founder, Tianqi Chen, in addition to the features described above, XGBoost's power also comes from its 'Block structures for parallel learning', 'Cache Awareness', and 'Out of Core Computing'. Data is sorted and stored in memory units called blocks. This allows the data to easily be re-used in later iterations, instead of the data having to be computed again. The data in each block is stored in a compressed column format, and with one scan of the block, the statistics for the split candidates can be found. Cache awareness is the existence of internal buffers where gradient stats can be stored. Out of core computing allows optimized disk space and maximized usage when dealing with datasets that are too large to fit in memory.</p>
<p>The one area where XGBoost is lacking is in feature engineering and hyper-parameter tuning. Chen, and the XGBoost team however, believe this is fine, as these areas are deeply integrated within coding programs. After reading documentation and several data science blogs on boosting systems, I see that there is an ability to grid search to find the best parameters. However, considering these boosting frameworks are designed for extremely large datasets, if we were to grid search one of these files, it would be computationally expensive, and time consuming. (Running a basic cross-validation ran almost 15 minutes). In order to improve a model, hyper-tuning is necessary, but it's hard to determine which parameters should be tuned and what the ideal value of each should be. </p>
<h1>Catboost</h1>
<p>CatBoost is another open-source gradient boosting on decision trees library. CatBoost's claim to fame is its categorical feature support. Instead of having to take care of one-hot-encoding during pre-processing the data, the CatBoost algorithm handles categorical columns while training the data. </p>
<p>I'm now going to use DataFrameMapper and StandardScaler to prepare the rest of the data. Following this, I will pass the categorical features, while fitting the model.</p>
<div class="highlight"><pre><span></span><span class="n">mapper2</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
    <span class="p">([</span><span class="s1">&#39;ra&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;dec&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;u&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;run&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;specobjid&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;redshift&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
<span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df_out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Z_train2</span> <span class="o">=</span> <span class="n">mapper2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_val2</span> <span class="o">=</span> <span class="n">mapper2</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># categorical_features = list(range(0, X.shape[1]))</span>
<span class="n">categorical_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Z_train2</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="nb">int</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">categorical_features</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[10 11 12 13]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model2</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">loss_function</span> <span class="o">=</span> <span class="s1">&#39;MultiClass&#39;</span><span class="p">,</span> <span class="c1">#For 2-class classification, we should use &quot;Logloss&quot; or &quot;Entropy&quot;</span>
    <span class="n">custom_metric</span><span class="o">=</span><span class="s1">&#39;AUC&#39;</span><span class="p">,</span>
    <span class="n">use_best_model</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span> <span class="c1">#information is printed out at every fifth iteration</span>
<span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">(</span><span class="n">Z_val2</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span> <span class="c1">#This should be True. But the printout is not compatible with this blog framework</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4231271</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span> <span class="o">(</span><span class="mi">0</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">108</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">10.7</span><span class="n">s</span>
<span class="mi">5</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0695255</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span> <span class="o">(</span><span class="mi">5</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">447</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mi">7</span><span class="n">s</span>
<span class="mi">10</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461552</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span> <span class="o">(</span><span class="mi">10</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mi">947</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.67</span><span class="n">s</span>
<span class="mi">15</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0407958</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0510322</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0508512</span> <span class="o">(</span><span class="mi">14</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.46</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.68</span><span class="n">s</span>
<span class="mi">20</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0350664</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span> <span class="o">(</span><span class="mi">20</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.99</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.48</span><span class="n">s</span>
<span class="mi">25</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0315475</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span> <span class="o">(</span><span class="mi">25</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.47</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.03</span><span class="n">s</span>
<span class="mi">30</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0272609</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span> <span class="o">(</span><span class="mi">30</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.96</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.6</span><span class="n">s</span>
<span class="mi">35</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0247762</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422198</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">3.52</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.26</span><span class="n">s</span>
<span class="mi">40</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0233293</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422688</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">4.09</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">5.88</span><span class="n">s</span>
<span class="n">Stopped</span> <span class="n">by</span> <span class="n">overfitting</span> <span class="n">detector</span>  <span class="o">(</span><span class="mi">10</span> <span class="n">iterations</span> <span class="n">wait</span><span class="o">)</span>

<span class="n">bestTest</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.04215835272</span>
<span class="n">bestIteration</span> <span class="o">=</span> <span class="mi">34</span>

<span class="n">Shrink</span> <span class="n">model</span> <span class="n">to</span> <span class="n">first</span> <span class="mi">35</span> <span class="n">iterations</span><span class="o">.</span>





<span class="o">&lt;</span><span class="n">catboost</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">CatBoostClassifier</span> <span class="n">at</span> <span class="mh">0x1c1e62d588</span><span class="o">&gt;</span>
</pre></div>


<p><img src='images/newplot1.png' alt='newplot1'></p>
<p>A great feature of CatBoost is it provides information at points specified in your params (above was 'verbose = 5').</p>
<div class="highlight"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">tree_count_</span>
</pre></div>


<div class="highlight"><pre><span></span>35
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span>
<span class="k">print</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Z_val2</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[[4.01704757e-04 9.80671060e-01 1.89272355e-02]</span>
 <span class="k">[3.08763482e-04 9.96511522e-01 3.17971463e-03]</span>
 <span class="k">[3.89049775e-04 9.98622853e-01 9.88097567e-04]</span>
 <span class="na">...</span>
 <span class="k">[9.97135061e-01 2.57287349e-03 2.92065282e-04]</span>
 <span class="k">[9.96680305e-01 2.99639694e-03 3.23297602e-04]</span>
 <span class="k">[2.87351792e-04 9.95101706e-01 4.61094214e-03]]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">predictions2</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Z_val2</span><span class="p">)</span>
</pre></div>


<p>Like XGBoost, CatBoost also includes an early stopping round. In my model, I set this parameter to 10. This means that if no improvement has been made after 10 trees, the model will stop running. This helps prevent against over fitting. CatBoost also, like XGBoost, has a built in cross-validation.</p>
<div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss_function&#39;</span><span class="p">:</span> <span class="s1">&#39;MultiClass&#39;</span><span class="p">,</span>
          <span class="s1">&#39;custom_loss&#39;</span><span class="p">:</span> <span class="s1">&#39;AUC&#39;</span>
         <span class="p">}</span>

<span class="n">cross_val_results2</span> <span class="o">=</span> <span class="n">cv</span><span class="p">(</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">Pool</span><span class="p">(</span><span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">),</span>
            <span class="n">fold_count</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="c1">#number of folds to split the data into.</span>
            <span class="n">inverted</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
            <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="c1">#data is randomly shuffled before splitting.</span>
            <span class="n">partition_random_seed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">stratified</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
            <span class="n">verbose</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">cross_val_results2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>iterations</th>
      <th>test-MultiClass-mean</th>
      <th>test-MultiClass-std</th>
      <th>train-MultiClass-mean</th>
      <th>train-MultiClass-std</th>
      <th>test-AUC:class=0-mean</th>
      <th>test-AUC:class=0-std</th>
      <th>test-AUC:class=1-mean</th>
      <th>test-AUC:class=1-std</th>
      <th>test-AUC:class=2-mean</th>
      <th>test-AUC:class=2-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-1.042713</td>
      <td>0.000341</td>
      <td>-1.042747</td>
      <td>0.000093</td>
      <td>0.992025</td>
      <td>0.003681</td>
      <td>0.985580</td>
      <td>0.003899</td>
      <td>0.982720</td>
      <td>0.005111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.992463</td>
      <td>0.001261</td>
      <td>-0.992504</td>
      <td>0.001639</td>
      <td>0.997906</td>
      <td>0.001791</td>
      <td>0.992262</td>
      <td>0.002721</td>
      <td>0.984138</td>
      <td>0.005245</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.946050</td>
      <td>0.000323</td>
      <td>-0.945929</td>
      <td>0.000795</td>
      <td>0.997925</td>
      <td>0.001205</td>
      <td>0.993606</td>
      <td>0.002700</td>
      <td>0.988726</td>
      <td>0.003737</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>-0.903349</td>
      <td>0.000100</td>
      <td>-0.903072</td>
      <td>0.000699</td>
      <td>0.998220</td>
      <td>0.001502</td>
      <td>0.993857</td>
      <td>0.003076</td>
      <td>0.988547</td>
      <td>0.003624</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>-0.863164</td>
      <td>0.000607</td>
      <td>-0.863020</td>
      <td>0.000325</td>
      <td>0.998217</td>
      <td>0.001357</td>
      <td>0.994078</td>
      <td>0.002699</td>
      <td>0.988690</td>
      <td>0.003612</td>
    </tr>
  </tbody>
</table>
</div>

<p>Also similar to XGBoost, CatBoost provides the user with feature importance. By calling on 'get_feature_importance', the user can see what the model learned about the features and which of them have the greatest influence on the decisions trees.</p>
<div class="highlight"><pre><span></span><span class="n">fstrs</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="n">prettified</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">{</span><span class="n">feature_name</span> <span class="p">:</span> <span class="n">value</span> <span class="k">for</span>  <span class="n">feature_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">fstrs</span><span class="p">}</span>
</pre></div>


<div class="highlight"><pre><span></span>{&#39;redshift&#39;: 78.69840467429411,
 &#39;i&#39;: 8.742787892612705,
 &#39;u&#39;: 3.487814704869871,
 &#39;specobjid&#39;: 2.0436652601559193,
 &#39;plate&#39;: 1.8955707345778037,
 &#39;z&#39;: 1.42498154722369,
 &#39;field&#39;: 0.9011905403991105,
 &#39;run&#39;: 0.7856686087514119,
 &#39;fiberid&#39;: 0.48419622387041755,
 &#39;dec&#39;: 0.40877000998928603,
 &#39;g&#39;: 0.3113040809109803,
 &#39;ra&#39;: 0.3112717293491661,
 &#39;r&#39;: 0.25950045766926433,
 &#39;camcol&#39;: 0.24487353532627085}
</pre></div>


<p>While XGBoost had the ability to use matplotlib to plot, Catboost's equivalent is brought to us by shap.</p>
<div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">Pool</span><span class="p">(</span><span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/catboost/core.py:1697: UserWarning: &#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead
  warnings.warn(&quot;&#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead&quot;)
The model has complex ctrs, so the SHAP values will be calculated approximately.
</pre></div>


<div class="highlight"><pre><span></span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span>
</pre></div>


<p>[0.10654885020023455, 1.5818176696303439, -1.6883665198535525]</p>
<div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">Z_train2</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="images/blog_3_48_0.png"></p>
<p>Above we've done a basic model, and we've discovered that redshift is the value that has the greatest effect on the decision trees. One of the best features about CatBoost is that we can now fine-tune parameters in order to speed up the model.</p>
<div class="highlight"><pre><span></span><span class="n">fast_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">random_seed</span><span class="o">=</span><span class="mi">62</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="c1">#The default is 1000 trees. Lowering the number of trees can speed up the model. </span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="c1">#Typically, if you lower the number of trees, you should raise the learning rate.</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;Plain&#39;</span><span class="p">,</span> <span class="c1">#The default is Ordered. This prevents overfitting but is expensive in computation.</span>
    <span class="n">bootstrap_type</span><span class="o">=</span><span class="s1">&#39;Bernoulli&#39;</span><span class="p">,</span> <span class="c1">#The default is Bayesian. Bernoulli is faster.</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="c1">#less than 1 is optimal for speed.</span>
    <span class="n">rsm</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="c1">#random subspace method speeds up the training</span>
    <span class="n">leaf_estimation_iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="c1">#for smaller datasets this should be set at 1 or 5.</span>
    <span class="n">max_ctr_complexity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#We could also set one_hot_max_size to larger than the default 2. The default 2 means that any category that has 2 </span>
<span class="c1">#or less values will be hot encoded. The rest will be computated by catboosts algorithm. This is more computationally</span>
<span class="c1">#expensive.</span>
<span class="n">fast_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c1e615080&gt;
</pre></div>


<p><img src='images/newplot2.png' alt='newplot2'></p>
<p>In addition to the above parameters that were adjusted to speed up the model's performance, there are also more parameters that can be used to fine-tune the model.</p>
<div class="highlight"><pre><span></span><span class="n">tuned_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="c1"># how deep trees will go. Usually best between 1 and 10</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
    <span class="n">l2_leaf_reg</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1">#regularizer value for Ridge regression</span>
    <span class="n">bagging_temperature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">one_hot_max_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1">#2 is the default value. </span>
    <span class="n">leaf_estimation_method</span><span class="o">=</span><span class="s1">&#39;Newton&#39;</span><span class="p">,</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s1">&#39;MultiClass&#39;</span>
<span class="p">)</span>
<span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">(</span><span class="n">Z_val2</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c400a10b8&gt;
</pre></div>


<p><img src='images/newplot3.png' alt='newplot3'></p>
<div class="highlight"><pre><span></span><span class="n">final_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">random_seed</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">tuned_model</span><span class="o">.</span><span class="n">tree_count_</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">final_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Learning rate set to 0.055965
0:  learn: 0.5431275    total: 61.7ms   remaining: 30.8s
100:    learn: 0.0057702    total: 6.14s    remaining: 24.3s
200:    learn: 0.0032198    total: 11.3s    remaining: 16.8s
300:    learn: 0.0021840    total: 16.3s    remaining: 10.8s
400:    learn: 0.0016674    total: 21.1s    remaining: 5.21s
499:    learn: 0.0013561    total: 25.5s    remaining: 0us





&lt;catboost.core.CatBoostClassifier at 0x1c1e2a59e8&gt;
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">get_best_score</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>{&#39;learn&#39;: {&#39;Logloss&#39;: 0.0013560782767138484}}
</pre></div>


<p>The greatest benefit of CatBoost is its ability to handle categorical features. With other boosting software, we need to preprocess data on our own. This would be done using LabelBinarizer (as shown above in the XGBoost section). Although DataFrameMapper makes this task more manageable, in some cases it may lead to datasets that are unimaginably large. In addition to combination features(combining columns), which are converted to number values (these combination features are not used in the first split, but then all will be used in the next splits), CatBoost has a special formula to convert categorical features into numbers. With CatBoost and its categorical features, the data is shuffled and a mean is calculated for every object on its historical data.</p>
<p>ctr i = (countInClass + prior) / (totalCount + 1)</p>
<p>CountInClass is how many times the label value was equal to i for objects with the current categorical feature value.</p>
<p>TotalCount is the total number of objects already visited that have the same feature value as the one being visited.</p>
<p>Prior is a constant defined by the starting parameters</p>
<p>When it comes to missing values, CatBoost also deals with these during training. The default setting is 'Min' which means that missing values are processed as the minimum value for the feature. It is also guaranteed that a split that seperates all missing values from all other values is considered when selecting trees.</p>
<h1>Conclusion</h1>
<p>In addition to categorical features, another main diference between XGBoost and CatBoost is the type of decision trees used. XGBoost uses assymetric trees (which could be viewed above) while CatBoost uses oblivious trees as base predictors. An assymetric tree makes splits that will learn the most. Oblivious trees simply means that the feature used for splitting is the same accross all intermediate nodes within the same level of the tree.</p>
<p>Overall, I find it's hard to choose the best boosting system, because both do a great job and offer great features to work with. This can be viewed by their use in winning Kaggle competition entries. However, I find CatBoost's ability to pass categorical features, instead of one-hot-encoding, novel. It saves time in preparing the data to be modeled and although more computationally expensive at first, I think with larger datasets, where one-hot-encoding could swell the dataset to enormous sizes, it would be more effective. I like the fact that it provides both options when you're fitting a model. The default is 2 - which means that if a categorical column has two or less categories it will hot encode, and if its more than 2, it will run its built-in calculation for categorical columns. Either way, categorical columns are processed internally when training the model. Thus, even if you prefer categorical columns to be one-hot-encoded, you can set the parameters accordingly and not have to pre-process these columns. </p>
<p>It's hard to choose between the two libraries because I think they both excel with larger datasets than the one I used. However, with my computer and the SkyServer dataset, CatBoost outperformed XGBoost in speed, and I found the ability to fine-tune and speed-up the model more user-friendly and intuitive. Also I found the instant plotting more engaging, and the shap add-on, more visually appealing.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_2.html" rel="bookmark">Don't be a Dummy, (For)get_dummies!</a></h2>
            <time datetime="2019-04-10T10:20:00-04:00">
                Apr 10, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>pd.get_dummies is a pandas function that transforms a column containing categorical variables into a series of new columns composed of 1s and 0s (true and false). The significance in this process is that in order for any model or algorithm to process categorical features, they must first be transformed into numerical representation. Why not assign categorical features a number between 1 and 10 and keep them in one column? This process would result in assigning irrelevant values to items in a categorical column that would be misinterpreted by an algorithm. More columns composed of 1s and 0s is preferred over a single column, because it has the benefit of categories not weighting a value improperly.</p>
<p>To show how Get_Dummies works, I will create a dictionary of hockey players and transform it into a DataFrame:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="p">({</span>

    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Sidney Crosby&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;31&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Pittsburgh Penguins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Nikita Kucherov&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;25&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Tampa Bay Lightning&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;RW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Connor McDavid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;22&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Edmonton Oilers&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Mikko Rantanen&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;22&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s2">&quot;Colorado Avalanche&quot;</span><span class="p">,</span> 
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;RW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Finnish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alex Ovechkin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;33&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Washington Capitals&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;LW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Braden Holtby&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;29&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Washington Capitals&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Brent Burns&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;34&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;San Jose Sharks&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;John Tavares&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;28&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Toronto Maple Leafs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span> 
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Mark Scheiele&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;26&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Winnipeg Jets&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Carey Price&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;31&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Montreal Canadians&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Morgan Rielly&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;25&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Toronto Maple Leafs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;American&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>

<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Nathan MacKinnon&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;23&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Colorado Avalanche&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Evgeni Malkin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;32&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Pittsburgh Penguins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hockey_players</span><span class="p">)</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;nationality&#39;</span><span class="p">,</span> <span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">,</span> <span class="s1">&#39;stanley_cup&#39;</span><span class="p">,</span> <span class="s1">&#39;hart&#39;</span><span class="p">]]</span>
<span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">HTML</span><span class="p">(</span><span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-stripped table-hover&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-stripped table-hover">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>age</th>
      <th>nationality</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sidney Crosby</td>
      <td>31</td>
      <td>Canadian</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Nikita Kucherov</td>
      <td>25</td>
      <td>Russian</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Connor McDavid</td>
      <td>22</td>
      <td>Canadian</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mikko Rantanen</td>
      <td>22</td>
      <td>Finnish</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Alex Ovechkin</td>
      <td>33</td>
      <td>Russian</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Above is a simple dataset consisiting of 13 National Hockey League players. For each player, we are provided with</p>
<div class="highlight"><pre><span></span>    -name 
    -age 
    -nationality 
    -team 
    -position 
    -whether he has won a Stanley Cup 
    -whether he has won the Hart trophy.
</pre></div>


<p>For the purpose of this blog, let's imagine that this is a much larger dataset consisting of all NHL players. As I mentioned above, in order to compare categorical variables, we need to transform them into 1s and 0s. </p>
<p>If a column is categorical and based on two inputs, forexample 'yes' and 'no', this can be as simple as: </p>
<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;stanley_cup&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;stanley_cup&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;yes&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span>                            OR
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>age</th>
      <th>nationality</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sidney Crosby</td>
      <td>31</td>
      <td>Canadian</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Nikita Kucherov</td>
      <td>25</td>
      <td>Russian</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Connor McDavid</td>
      <td>22</td>
      <td>Canadian</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mikko Rantanen</td>
      <td>22</td>
      <td>Finnish</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Alex Ovechkin</td>
      <td>33</td>
      <td>Russian</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>Whenever the column is more complex, this is when someone would reach for pd.get_dummies.</p>
<p>Say we thought that the nationality of a player was indicative of performance. In order to use this categorical value in a model, we would need to transform these values into 1s and 0s. </p>
<p>Now I am going to show how to use pd.get_dummies to transform the nationality column into multiple columns that correspond with the nationality of each player. </p>
<div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;nationality&#39;</span><span class="p">])</span>
<span class="n">dummy</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>The function provided us with 4 columns each representing 1 of 4 nationalities in the dataset. In order to compare this information to the original dataset, we will concatenate onto the original DataFrame. While we are doing that, we will drop the original column for nationality, as well as the name category, as each player is already identfied by an id number.</p>
<div class="highlight"><pre><span></span><span class="n">hockey_players</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hockey_players</span><span class="p">,</span> <span class="n">dummy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;nationality&#39;</span><span class="p">])</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">])</span>
<span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>At first sight, this seems like a very useful tool provided by pandas. We can now compare the nationality of each player to the values in the other columns.</p>
<p>However, there is one very large downside to using pd.get_dummies, which I will demonstrate below.</p>
<p>Imagine that we were attempting to see whether all these factors could be used to make a model that predicted whether a player would win the Hart trophy. First we will identify our features and our target.</p>
<div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">cols</span> <span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">hockey_players</span> <span class="k">if</span> <span class="n">cols</span> <span class="o">!=</span> <span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span>
</pre></div>


<p>Since the dataset is clean (no missing values), the next thing we will do is split the data into training and testing sets.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<p>Now we have two sets of data:</p>
<div class="highlight"><pre><span></span>-One to train our model on (X_train)
-One to test our final model on (X_test)
</pre></div>


<p>Next we look at our X_train and we see that we will need to use pd.get_dummies on the categorical columns for position and team. </p>
<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>team_Montreal Canadians</th>
      <th>team_Pittsburgh Penguins</th>
      <th>team_Tampa Bay Lightning</th>
      <th>team_Toronto Maple Leafs</th>
      <th>team_Washington Capitals</th>
      <th>position_D</th>
      <th>position_G</th>
      <th>position_LW</th>
      <th>position_RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>29</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Next, we will do the same thing for the testing data.</p>
<div class="highlight"><pre><span></span><span class="n">X_test_with_dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>Now with our training and testing data hot-encoded, we are ready to model! Let's instantiate the Logistic Regression and fit and score our model.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_with_dummy</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_with_dummy</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">linear_model</span><span class="o">/</span><span class="n">logistic</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">433</span><span class="p">:</span> <span class="ne">FutureWarning</span><span class="p">:</span> <span class="n">Default</span> <span class="n">solver</span> <span class="n">will</span> <span class="n">be</span> <span class="n">changed</span> <span class="n">to</span> <span class="s1">&#39;lbfgs&#39;</span> <span class="ow">in</span> <span class="mf">0.22</span><span class="o">.</span> <span class="n">Specify</span> <span class="n">a</span> <span class="n">solver</span> <span class="n">to</span> <span class="n">silence</span> <span class="n">this</span> <span class="n">warning</span><span class="o">.</span>
  <span class="ne">FutureWarning</span><span class="p">)</span>



<span class="o">---------------------------------------------------------------------------</span>

<span class="ne">ValueError</span>                                <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">13</span><span class="o">-</span><span class="mi">7057</span><span class="n">dac4b7c1</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
      <span class="mi">4</span> 
      <span class="mi">5</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_with_dummy</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="o">----&gt;</span> <span class="mi">6</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_with_dummy</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>


<span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">base</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="n">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="mi">286</span>         <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    287         from .metrics import accuracy_score</span>
<span class="s2">--&gt; 288         return accuracy_score(y, self.predict(X), sample_weight=sample_weight)</span>
<span class="s2">    289 </span>
<span class="s2">    290</span>


<span class="s2">/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py in predict(self, X)</span>
<span class="s2">    279             Predicted class label per sample.</span>
<span class="s2">    280         &quot;&quot;&quot;</span>
<span class="o">--&gt;</span> <span class="mi">281</span>         <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="mi">282</span>         <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="mi">283</span>             <span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">linear_model</span><span class="o">/</span><span class="n">base</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="n">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="mi">260</span>         <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
    <span class="mi">261</span>             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X has </span><span class="si">%d</span><span class="s2"> features per sample; expecting </span><span class="si">%d</span><span class="s2">&quot;</span>
<span class="o">--&gt;</span> <span class="mi">262</span>                              <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_features</span><span class="p">))</span>
    <span class="mi">263</span> 
    <span class="mi">264</span>         <span class="n">scores</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>


<span class="ne">ValueError</span><span class="p">:</span> <span class="n">X</span> <span class="n">has</span> <span class="mi">9</span> <span class="n">features</span> <span class="n">per</span> <span class="n">sample</span><span class="p">;</span> <span class="n">expecting</span> <span class="mi">15</span>
</pre></div>


<p>VALUE ERROR! </p>
<p>When we used pd.get_dummies to make dummy columns that were hot-encoded, we over-looked the fact that the testing data was much smaller and it did not have examples for each categorical variable. Although we performed pd.get_dummies on the testing data, as we did on the training data, there was no way for the transformed testing data to know that it was missing certain columns that were present in the original data set and still existed in the training data.</p>
<p>Observe the shapes of our two data sets:</p>
<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(10, 15)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_test_with_dummy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(3, 9)
</pre></div>


<p>Some may argue that this could easily be avoided by using pd.get_dummies before splitting the data into training and testing sets. However, every good Data Scientist knows the importance of using train_test_split first before tampering with your dataset.</p>
<p>These two ideas seem at odds, because they are! What to do?</p>
<p>Luckily SkLearn provides us with a better and more advanced function which considers this possible dilemna. Because of this, SkLearn's Label Binarizer should be used instead of pd.get_dummies.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">cols</span> <span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">hockey_players</span> <span class="k">if</span> <span class="n">cols</span> <span class="o">!=</span> <span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">])</span>
<span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>      <span class="c1">#This is where the magic happens!</span>
<span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">])</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_train</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>29</td>
      <td>Washington Capitals</td>
      <td>G</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>34</td>
      <td>San Jose Sharks</td>
      <td>D</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>0</th>
      <td>31</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>The built in method call 'classes_' is the saviour in this function. By fitting the training data and than labeling the columns based on the method call 'classes_', the existence of all 5 positions is remembered. Simply put, the attribute 'classes_' holds the label for each class. Now we will transform our test data.</p>
<div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_test</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>28</td>
      <td>Toronto Maple Leafs</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>26</td>
      <td>Winnipeg Jets</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>31</td>
      <td>Montreal Canadians</td>
      <td>G</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>As you can see, even though none of the players in the testing data played wing, instantiating Label Binarizer and utilizing the method 'classes_' ensures that these columns were preserved in the testing data.</p>
<p>Now let's perform LabelBinarizer on the team column as well!</p>
<div class="highlight"><pre><span></span><span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">])</span>
<span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>     
<span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">])</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
      <th>Colorado Avalanche</th>
      <th>Edmonton Oilers</th>
      <th>Pittsburgh Penguins</th>
      <th>San Jose Sharks</th>
      <th>Tampa Bay Lightning</th>
      <th>Toronto Maple Leafs</th>
      <th>Washington Capitals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>    <span class="c1">#Remembering the classes created from the original categorical column.</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">])</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">])</span>
</pre></div>


<p>Now let's see if we can model our data...</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)





0.6666666666666666
</pre></div>


<p>SUCCESS THEY'RE THE SAME SIZE!!!</p>
<p>LabelBinarizer is better than pd.get_dummies because when you fit your training data it gives you back a class_ method which remembers the different categories that were created when you hot encoded your column into multiple columns. When you transform your testing data and call the class attributes that were stored when you fit your training data, even though your test data may not have samples from that specific column, it remembers that it needs to represent the column even though there are no 1s present.</p>
<div class="highlight"><pre><span></span>
</pre></div>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_1.html" rel="bookmark">The Functionality of Functions</a></h2>
            <time datetime="2019-03-28T10:20:00-04:00">
                Mar 28, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p><em>The Functionality of Functions</em></p>
<p>Coming from an academic background based in History, Philosophy, and Law, learning Data Science can be  overwhelming; especially when you haven't taken a Computer or Math course in TWELVE years. The biggest issues I have had in the first three weeks deal with syntax. Learning Python is the equivalent of learning a new language. The one area I have continued to struggle with is how to develop new functions. While this is by far not the hardest task thrown at us, it is essential to most tasks a Data Scientist faces daily.</p>
<p>To start, writing a basic function is very simple.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_my_name</span><span class="p">(</span><span class="n">first_name</span><span class="p">,</span> <span class="n">last_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">last_name</span><span class="p">,</span> <span class="n">first_name</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">print_my_name</span><span class="p">(</span><span class="s1">&#39;kristi&#39;</span><span class="p">,</span> <span class="s1">&#39;gourlay&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(&#39;gourlay&#39;, &#39;kristi&#39;)
</pre></div>


<p>OR</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adder</span><span class="p">(</span><span class="n">number1</span><span class="p">,</span> <span class="n">number2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">number1</span> <span class="o">+</span> <span class="n">number2</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">adder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>5
</pre></div>


<p>They get slightly more difficult when you're asked to count something or create a list. </p>
<p>Consider the following function that counts the vowels in a provided word.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vowel_counter</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vowels</span> <span class="o">=</span> <span class="s1">&#39;aeiou&#39;</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">vowels</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">return</span> <span class="n">counter</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">vowel_counter</span><span class="p">(</span><span class="s1">&#39;elephant&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>1
</pre></div>


<p>WAIT! That's not right!</p>
<p>Lesson One: Make sure the return is outside the loop!
This placement of the return call had me pulling my hair out for the entire first week. I could not understand why my functions, that looked just like my classmates' functions, were not working. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vowel_counter</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vowels</span> <span class="o">=</span> <span class="s1">&#39;aeiou&#39;</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">vowels</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">counter</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">vowel_counter</span><span class="p">(</span><span class="s1">&#39;elephant&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>3
</pre></div>


<p>While writing a simple function is easy, the next step is learning to write a function that can be used on data or a dictionary. I struggled with this in our first lab.</p>
<p>Below I have created a dictionary of tv shows:</p>
<div class="highlight"><pre><span></span><span class="n">tvshows</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;curb your enthusiasm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the wire&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;shameless&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;dramedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;showtime&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the sopranos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;game of thrones&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;house of cards&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;netflix&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;kimmy schmidt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;netflix&#39;</span>
<span class="p">}]</span>

<span class="k">print</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[{&#39;name&#39;: &#39;curb your enthusiasm&#39;, &#39;category&#39;: &#39;comedy&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;the wire&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;shameless&#39;, &#39;category&#39;: &#39;dramedy&#39;, &#39;network&#39;: &#39;showtime&#39;}, {&#39;name&#39;: &#39;the sopranos&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;game of thrones&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;house of cards&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;netflix&#39;}, {&#39;name&#39;: &#39;kimmy schmidt&#39;, &#39;category&#39;: &#39;comedy&#39;, &#39;network&#39;: &#39;netflix&#39;}]
</pre></div>


<p>For example, I want to make a function to iterate through the dictionary and return all the shows that are from a specific network.</p>
<p>The first step is to create a 'for loop' OR use a 'list comprehension' to write what you want to get back. </p>
<p>In this instance, I would like to see all the tv show names that are on the HBO network.</p>
<p>Name a function and pass it two arguments. 
1)what you are going to iterate through.
2)what you are looking for.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tv_net</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="s1">&#39;tvshows&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;hbo&#39;</span><span class="p">):</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">tv</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">tv</span> <span class="ow">in</span> <span class="n">dictionary</span> <span class="k">if</span> <span class="n">tv</span><span class="p">[</span><span class="s1">&#39;network&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">network</span><span class="p">]</span>

<span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;curb your enthusiasm&#39;, &#39;the wire&#39;, &#39;the sopranos&#39;, &#39;game of thrones&#39;]
</pre></div>


<p>Since I set the second argument default to hbo, the function automatically looks for shows from the HBO network. Setting a default argument makes life easier, and helps you remember what type of argument you will be looking for. If I wanted to change the network argument, it's as simple as passing a different network through.</p>
<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;netflix&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;house of cards&#39;, &#39;kimmy schmidt&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;showtime&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;shameless&#39;]
</pre></div>


<p>The important thing I learned from this process was the fact that you can then take the function you made and use it on similar data later in your code. For example, say I was then provided with a dictionary of prime time tv shows.</p>
<div class="highlight"><pre><span></span><span class="n">primetime</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;this is us&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;family guy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;fox&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the good place&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the office&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">}</span>
<span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">primetime</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;nbc&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;this is us&#39;, &#39;the good place&#39;, &#39;the office&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">primetime</span><span class="p">,</span> <span class="s1">&#39;fox&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;family guy&#39;]
</pre></div>


<p>This importance of using functions to iterate through data and dictionaries resurfaces in EDA. If we are looking at a set of data and need to change something about it, and we may need to make similar changes later in our code, it's practical to create a function.
To show this, I will use the tvshow sets from above. (First I will need to place them in a Data Frame)</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">tvshows</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
<span class="n">primetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">primetime</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tvshows</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category</th>
      <th>name</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>comedy</td>
      <td>curb your enthusiasm</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>1</th>
      <td>drama</td>
      <td>the wire</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>2</th>
      <td>dramedy</td>
      <td>shameless</td>
      <td>showtime</td>
    </tr>
    <tr>
      <th>3</th>
      <td>drama</td>
      <td>the sopranos</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>drama</td>
      <td>game of thrones</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>5</th>
      <td>drama</td>
      <td>house of cards</td>
      <td>netflix</td>
    </tr>
    <tr>
      <th>6</th>
      <td>comedy</td>
      <td>kimmy schmidt</td>
      <td>netflix</td>
    </tr>
  </tbody>
</table>
</div>

<p>When we look at the dataframe for tvshows, we notice that the category and the name columns need to be swapped. We could run a simple list command, but since we know we have other similar data, it may be easier to create a function so that we don't have to repeat these verbose commands later.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">swap_col</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cols</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">),</span> <span class="n">cols</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
    <span class="n">cols</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
    <span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dataframe</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">swap_col</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>category</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>curb your enthusiasm</td>
      <td>comedy</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>1</th>
      <td>the wire</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>2</th>
      <td>shameless</td>
      <td>dramedy</td>
      <td>showtime</td>
    </tr>
    <tr>
      <th>3</th>
      <td>the sopranos</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>game of thrones</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>5</th>
      <td>house of cards</td>
      <td>drama</td>
      <td>netflix</td>
    </tr>
    <tr>
      <th>6</th>
      <td>kimmy schmidt</td>
      <td>comedy</td>
      <td>netflix</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now we import our other dataframe and we see the same item that needs to be fixed. </p>
<div class="highlight"><pre><span></span><span class="n">primetime</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category</th>
      <th>name</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>drama</td>
      <td>this is us</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>1</th>
      <td>comedy</td>
      <td>family guy</td>
      <td>fox</td>
    </tr>
    <tr>
      <th>2</th>
      <td>comedy</td>
      <td>the good place</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>3</th>
      <td>comedy</td>
      <td>the office</td>
      <td>nbc</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">swap_col</span><span class="p">(</span><span class="n">primetime</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>category</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this is us</td>
      <td>drama</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>1</th>
      <td>family guy</td>
      <td>comedy</td>
      <td>fox</td>
    </tr>
    <tr>
      <th>2</th>
      <td>the good place</td>
      <td>comedy</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>3</th>
      <td>the office</td>
      <td>comedy</td>
      <td>nbc</td>
    </tr>
  </tbody>
</table>
</div>

<p>The ability to make a function to fix mistakes in your data becomes a useful skill to have. It will be a time saver, and that's one of the reasons why learning functions is quite important for Data Science. </p>
<p>One last point.</p>
<p>I've shown how functions become important when both analyzing and fixing your data. Sometimes you might need your function to include multiple steps and the sheer volume may seem daunting. The number one tip, I have taken away in my first three weeks learning Python is to break things down. If you take everything step by step and work your way up to the return you need, the task becomes much easier.</p>
<p>It's easier to do many smaller pieces of the whole, than to tackle the whole.</p>
<p>STEP ONE: DONT GET OVERWHELMED!</p>
<p>STEP TWO: SIMPLY WRITE DOWN WHAT THE FUNCTION NEEDS TO DO/RETURN</p>
<p>STEP THREE: WRITE DOWN IN ORDER WHAT YOU NEED TO DO TO GET THE DESIRED RETURN</p>
<p>The perfect example of this breaking down a function was presented to me in our first project. We were asked to analyze SAT and ACT data. At one point in the assignment, we were asked to design a function that returns the standard deviation for a column. This question took me 10x longer than any other question, because I kept thinking about the final product. Eventually, I realised I needed to follow the three steps laid out above.</p>
<p>After calming myself down, I answered STEP TWO. The function needs to return the standard deviation of a column in the SAT/ACT dataset. </p>
<p>STEP THREE: How do you calculate standard deviation?</p>
<ol>
<li>
<p>Calculate the mean</p>
<p>mean = sum(column) / len(column)</p>
</li>
<li>
<p>For each number, subtract the mean and square the results</p>
<div class="highlight"><pre><span></span>((n - (sum(column) / len(column)))**2)
</pre></div>


</li>
<li>
<p>Find the mean of those squared differences( START A LIST before the math. new_list = [] )</p>
<p>new_list.append(((n - (sum(column) / len(column)))**2))</p>
</li>
<li>
<p>Take the square root of that list</p>
<p>math.sqrt(((sum(new_list))) / (len(column) - 1))</p>
</li>
</ol>
<p>Start with the first thing you need to do and slowly add what you need next. The result is:   </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">calc_std</span><span class="p">(</span><span class="n">column</span><span class="p">):</span>
    <span class="n">new_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">column</span><span class="p">:</span>
        <span class="n">new_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">column</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">column</span><span class="p">)))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="nb">sum</span><span class="p">(</span><span class="n">new_list</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">column</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">column</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">calc_std</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>2.416461403433896
</pre></div>


<p>Writing functions was something that I alone seemed to struggle with in my cohort. However, my focus on improving my ability to write functions, allowed me to highlight other areas in my work that I should keep an eye on. If you're code is not working, read the return for details, if you still cannot figure out what's wrong, double check these few things:
* Placement
* Accuracy
* Brackets!</p>
<p>Placement: Make sure your return is located outside the loop. But also make sure that lists and counters are located in the right place as well. If needed for the function, place the lists right below the function call.</p>
<p>Accuracy: The amount of times my code was not working because I mis-spelt a word in one place. Spelling will always be important, not just in academia!</p>
<p>Brackets: If in doubt look if you're missing a bracket somewhere. I spent an hour with a function not working, to realize I just needed to square bracket the argument I was submitting.</p>
<p>Attention to detail!</p>
<div class="highlight"><pre><span></span>
</pre></div>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="span7 offset2">
    <p class="paginator">
        Page 1 / 1
    </p>
</div>
    </div>
                        </div>
                    </div> <!-- content row -->

                    <div class="row"> <!-- footer -->
                        <div class="span7 offset2">
                            <footer id="site-footer">
                                <p>
                                    <a href="" target="_blank">
                                        
                                    </a> Kristi Gourlay /
                                    <a href="https://KristiGourlay.github.io/blog/">Archives</a>
                                </p>
                                <p>
                                    Powered by
                                    <a href="https://github.com/getpelican/pelican" target="_blank">
                                        Pelican
                                    </a>
                                    /
                                    Source code on
                                    <a href="http://github.com/siovene/iovene.com" target="blank">
                                        GitHub
                                    </a>
                                    /
                                    Theme <code>Lannisport</code>, also on
                                    <a href="http://github.com/siovene/lannisport" target="_blank">
                                        GitHub
                                    </a>
                                </p>
                            </footer>
                        </div> <!-- footer span -->
                    </div> <!-- footer row -->
                </div> <!-- header+content+footer span -->

                <div class="span3">
                    <aside class="affix">
                        <div id="sidebar">
                            <div id="social-icons">
                            </div>

                            <div id="navigation">
<nav>
	<h2>Pages</h2>
	<ul>
		<li>
			<ul>
				<li>
					<a href="https://KristiGourlay.github.io/blog">Blog</a>
				</li>

			</ul>
		</li>
	</ul>
</nav>

<nav>
	<h2>Categories</h2>
	<ul>
		<li>
			<ul>
					<li>
						<a href="https://KristiGourlay.github.io/blog/category/misc.html">misc</a>
					</li>
			</ul>
		</li>
	</ul>
</nav>                            </div>
                        </div> <!-- sidebar -->

                        <p id="back-to-top">
                            <a href="#">
                                <i class="icon-double-angle-up"></i> Back to top
                            </a>
                        </p>

                    </aside>
                </div> <!-- logo+navigation span -->
            </div> <!-- row -->

        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>

        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/js/bootstrap.min.js"></script>
        <script>$('body').popover || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/bootstrap.min.js"><\/script>')</script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/jquery.captions.js"></script>
        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery.colorbox-min.js"></script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/main.js"></script>

        <script>
            var _gaq=[['_setAccount',''],['_trackPageview'],['_trackPageLoadTime'],['_gat._anonymizeIp']];
            (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
            g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
            s.parentNode.insertBefore(g,s)}(document,'script'));
        </script>
    </body>
</html>