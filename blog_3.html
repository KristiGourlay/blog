<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Kitty Got Claws! A Comparison of CatBoost and XGBoost / Kristi's Blog</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="shortcut icon" href="https://KristiGourlay.github.io/blog/theme/favicon.ico" />
        <link rel="apple-touch-icon" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone4.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad3.png" />

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/colorbox.css">

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/main.css">

        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    </head>
    <body>

        <div class="container">
            <div class="row">
                <div class="span9">
                    <div class="row">
                        <div class="span2"> <!-- logo -->
                            <div id="logo">
                                <a href="https://KristiGourlay.github.io/blog">
                                    <img alt="Kristi's Blog" src="https://KristiGourlay.github.io/blog/" />
                                </a>
                            </div> <!-- logo -->
                        </div> <!-- logo span -->

                        <div class="span7"> <!-- header -->
                            <!--[if lt IE 9]>
                                <p class="chromeframe alert alert-warning">You are using an <strong>outdated</strong> browser, and this site might not look best in it. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
                            <![endif]-->
                            <header id="site-header">
                                <div id="site-header-content">
                                    <h1>
                                        <a href="https://KristiGourlay.github.io/blog">Kristi's Blog</a>
                                        <small>
                                            <span class="divider">/</span> 
                                        </small>
                                    </h1>
                                    <p>
                                        
                                    </p>
                                </div>
                            </header>
                        </div> <!-- header span -->
                    </div> <!-- header row -->

                    <div class="row"> <!-- content -->
                        <div class="content">
 <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_3.html" rel="bookmark">Kitty Got Claws! A Comparison of CatBoost and XGBoost</a></h2>
            <time datetime="2019-04-23T10:20:00-04:00">
                Apr 23, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    <div class="article-meta article-meta-border">
        <dl class="dl-horizontal">
            <dt><i class="icon-file"></i></dt>
            <dd>
                <a href="https://KristiGourlay.github.io/blog/category/misc.html">
                    misc
                </a>
            </dd>

            <dt><i class="icon-tags"></i></dt>
            <dd>
                    <a href="https://KristiGourlay.github.io/blog/tag/python.html">python</a>
            </dd>
        </dl>

        <b class="border-notch notch"></b>
        <b class="notch"></b>
    </div>
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <p>Boosting is an ensemble technique where new models are created to correct the errors of past models. Each subsequent tree learns from the errors of its predecessor, as all trees are added sequentially until no further improvements can be made. Gradient boosting is an approach where new models are created from the residuals or errors of prior models and then are added together to make the final prediction. The base learners in gradient boosting are weak learners, but each contributes vital information for the final model. Two examples of these programs are XGBoost and CatBoost. They are two open-source software libraries that provide a gradient boosting framework. </p>
<p>To compare these two programs, I will load a dataset called "Skyserver", and I will attempt to predict whether an image is a Star, Galaxy, or Quasar. I will begin with XGBoost.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span><span class="p">,</span> <span class="n">FunctionTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">import</span> <span class="nn">catboost</span> <span class="kn">as</span> <span class="nn">cb</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">Pool</span>
<span class="kn">import</span> <span class="nn">shap</span>

<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_tree</span><span class="p">,</span> <span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">plot_importance</span>


<span class="n">sky</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Skyserver.csv&#39;</span><span class="p">)</span>
</pre></div>


<p>The first thing I do with a dataset is some basic EDA. This includes checking to see whether there are null values: sky.isnull().sum(). The Skyserver dataset is very clean and there are no null values. However, with XGBoost, even if there were null values, XGBoost has a built in algorithm to impute these missing values. Its algorithm automatically learns what is the best imputation value for missing values based on training. This is one key advantage of using XGBoost. </p>
<p>Before I am ready to model, I will prepare my dataset, by labeling the 3 items in the target column - 'star', 'galaxy', 'qso'. Then I will drop unnecessary columns, and assign my y(target) and X(features).</p>
<div class="highlight"><pre><span></span><span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;STAR&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;GALAXY&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;QSO&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sky</span> <span class="o">=</span> <span class="n">sky</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;objid&#39;</span><span class="p">,</span> <span class="s1">&#39;rerun&#39;</span><span class="p">,</span> <span class="s1">&#39;mjd&#39;</span><span class="p">])</span> 
<span class="c1">#drop &#39;objid&#39; because irrelevant to model and &#39;rerun&#39; because it has the same value for each row.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">sky</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sky</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">data_dmatrix</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">)</span> 
<span class="c1">#DMatrix is an internal data structure used in XGBoost. It is recommended as it is optimal for memory and speed.</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, &#39;base&#39;, None) is not None and \
/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version
  data.base is not None and isinstance(data, np.ndarray) \
</pre></div>


<p>Having completed basic EDA, assigned our X and y, and created a data dmatrix, we are ready to split out data into training and testing sets and use DataFrameMapper, StandardScaler and LabelBinarizer to prepare our data. We do not need to worry about imputing at this point, because as stated above, in addition to the dataset being clean of missing values, XGBoost has a built in imputer that deals with missing values.</p>
<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<p>Obviously a key difference between XGBoost and CatBoost is how each deals with categorical columns. I'll discuss CatBoost later, but for now, as is shown below, for XGBoost, categorical columns need to be one hot encoded, using LabelBinarizer. Knowing that creating dummy columns, in many instances, could expand datasets to unimaginable sizes, the XGBoost developers introduced a built-in method to deal with this, called Automatic Sparse Data Optimization. The developers chose to handle this in the same way that they handled the existence of missing values, by making the algorithm aware of the sparsity pattern in the dataset. By only visiting the present values, and not the missing values and zeroes; it exploits the sparsity and learns the best directions to handle missing values. It handles all cases of sparsity in a unified way. </p>
<div class="highlight"><pre><span></span><span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
    <span class="p">([</span><span class="s1">&#39;ra&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;dec&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;u&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;run&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;camcol&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;specobjid&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;redshift&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;plate&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;fiberid&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
<span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df_out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Z_train</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_val</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">DataConversionWarning</span><span class="p">)</span>
</pre></div>


<p>Now it's time to model, predict, and score!</p>
<div class="highlight"><pre><span></span><span class="n">model1</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="c1">#How many trees.</span>
    <span class="n">num_class</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="c1">#Identifying there are 3 classes in our target</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="s1">&#39;reg:logistic&#39;</span><span class="p">,</span> <span class="c1">#Objective refers to which model you want to use.</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="c1">#Eval_set refers to how you want your model to be evaluated.</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1">#Learning_rate has to do with how much you want each tree to learn.</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1">#stop after ten iterations if no improvements have been made.            </span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1">#Smaller trees are more interpretable</span>
    <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1">#Silent deals with whether we want information for each iteration to be printed out.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
       colsample_bytree=1, early_stopping_rounds=10, eval_set=&#39;Accuracy&#39;,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
       min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,
       nthread=None, num_class=3, objective=&#39;multi:softprob&#39;,
       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
       seed=None, silent=False, subsample=1)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z_val</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.01
</pre></div>


<p>Another key feature is that there is a built-in cross validation. This runs a cross validation at each iteration of the boosting process.</p>
<div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span> <span class="s1">&#39;0.3&#39;</span><span class="p">,</span> <span class="c1">#the fraction of columns to be randomly sampled for each tree.</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="s1">&#39;0.2&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="c1">#the maximum depth of a tree</span>
        <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="s1">&#39;10&#39;</span><span class="p">}</span> <span class="c1">#L1 regularization weight - Lasso</span>

<span class="n">cross_val_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">data_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span> <span class="c1">#this is where the dmatrix comes in handy.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">cross_val_results</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train-rmse-mean</th>
      <th>train-rmse-std</th>
      <th>test-rmse-mean</th>
      <th>test-rmse-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.615842</td>
      <td>0.001887</td>
      <td>0.617878</td>
      <td>0.001618</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.514400</td>
      <td>0.023372</td>
      <td>0.517281</td>
      <td>0.024252</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.456278</td>
      <td>0.013461</td>
      <td>0.460037</td>
      <td>0.015424</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.420923</td>
      <td>0.006550</td>
      <td>0.426109</td>
      <td>0.009830</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.385381</td>
      <td>0.030245</td>
      <td>0.391525</td>
      <td>0.033427</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">((</span><span class="n">cross_val_results</span><span class="p">[</span><span class="s2">&quot;test-rmse-mean&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>9    0.281167
Name: test-rmse-mean, dtype: float64
</pre></div>


<div class="highlight"><pre><span></span><span class="n">xgreg</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="o">=</span><span class="n">data_dmatrix</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>


<p>A benefit of using boosting is that it provides you with feature importance. It provides you with a score which indicates how valuable a feature was in the construction of the boosted decision tree. The more a certain feature is used to make key decisions in the decision tree, the higher its importance. Importance is calculated by the the amount that each split point improves the overall tree, weighted over the number of observations it is resposible for.</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="n">plot_importance</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/blog_3_23_0.png"></p>
<p>As can be viewed by the chart, Redshift is by the far the most important feature.</p>
<p>XGBoost also provides the ability to peak into your model and inspect a tree. </p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">xgreg</span><span class="p">,</span> <span class="n">num_trees</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="images/blog_3_25_0.png"></p>
<p>Above you can see how a decision tree on this data looks. You can see that it is assymetrical, which is an important point of comparison with CatBoost's trees. One interesting aspect about this tree is the fact that you can see that the algorithm has chosen to impute all 'missing' values with the direction of yes. </p>
<p>According to XGBoost's documentation, website, and papers given by founder, Tianqi Chen, in addition to the features described above, XGBoost's power also comes from its 'Block structures for parallel learning', 'Cache Awareness', and 'Out of Core Computing'. Data is sorted and stored in memory units called blocks. This allows the data to easily be re-used in later iterations, instead of the data having to be computed again. The data in each block is stored in a compressed column format, and with one scan of the block, the statistics for the split candidates can be found. Cache awareness is the existence of internal buffers where gradient stats can be stored. Out of core computing allows optimized disk space and maximized usage when dealing with datasets that are too large to fit in memory.</p>
<p>The one area where XGBoost is lacking is in feature engineering and hyper-parameter tuning. Chen, and the XGBoost team however, believe this is fine, as these areas are deeply integrated within coding programs. After reading documentation and several data science blogs on boosting systems, I see that there is an ability to grid search to find the best parameters. However, considering these boosting frameworks are designed for extremely large datasets, if we were to grid search one of these files, it would be computationally expensive, and time consuming. (Running a basic cross-validation ran almost 15 minutes). In order to improve a model, hyper-tuning is necessary, but it's hard to determine which parameters should be tuned and what the ideal value of each should be. </p>
<h1>Catboost</h1>
<p>CatBoost is another open-source gradient boosting on decision trees library. CatBoost's claim to fame is its categorical feature support. Instead of having to take care of one-hot-encoding during pre-processing the data, the CatBoost algorithm handles categorical columns while training the data. </p>
<p>I'm now going to use DataFrameMapper and StandardScaler to prepare the rest of the data. Following this, I will pass the categorical features, while fitting the model.</p>
<div class="highlight"><pre><span></span><span class="n">mapper2</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
    <span class="p">([</span><span class="s1">&#39;ra&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;dec&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;u&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;run&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;specobjid&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;redshift&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
<span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df_out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Z_train2</span> <span class="o">=</span> <span class="n">mapper2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_val2</span> <span class="o">=</span> <span class="n">mapper2</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># categorical_features = list(range(0, X.shape[1]))</span>
<span class="n">categorical_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Z_train2</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="nb">int</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">categorical_features</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[10 11 12 13]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model2</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">loss_function</span> <span class="o">=</span> <span class="s1">&#39;MultiClass&#39;</span><span class="p">,</span> <span class="c1">#For 2-class classification, we should use &quot;Logloss&quot; or &quot;Entropy&quot;</span>
    <span class="n">custom_metric</span><span class="o">=</span><span class="s1">&#39;AUC&#39;</span><span class="p">,</span>
    <span class="n">use_best_model</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span> <span class="c1">#information is printed out at every fifth iteration</span>
<span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">(</span><span class="n">Z_val2</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span> <span class="c1">#This should be True. But the printout is not compatible with this blog framework</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4231271</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span> <span class="o">(</span><span class="mi">0</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">108</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">10.7</span><span class="n">s</span>
<span class="mi">5</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0695255</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span> <span class="o">(</span><span class="mi">5</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">447</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mi">7</span><span class="n">s</span>
<span class="mi">10</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461552</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span> <span class="o">(</span><span class="mi">10</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mi">947</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.67</span><span class="n">s</span>
<span class="mi">15</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0407958</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0510322</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0508512</span> <span class="o">(</span><span class="mi">14</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.46</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.68</span><span class="n">s</span>
<span class="mi">20</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0350664</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span> <span class="o">(</span><span class="mi">20</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.99</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.48</span><span class="n">s</span>
<span class="mi">25</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0315475</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span> <span class="o">(</span><span class="mi">25</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.47</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.03</span><span class="n">s</span>
<span class="mi">30</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0272609</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span> <span class="o">(</span><span class="mi">30</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.96</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.6</span><span class="n">s</span>
<span class="mi">35</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0247762</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422198</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">3.52</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.26</span><span class="n">s</span>
<span class="mi">40</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0233293</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422688</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">4.09</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">5.88</span><span class="n">s</span>
<span class="n">Stopped</span> <span class="n">by</span> <span class="n">overfitting</span> <span class="n">detector</span>  <span class="o">(</span><span class="mi">10</span> <span class="n">iterations</span> <span class="n">wait</span><span class="o">)</span>

<span class="n">bestTest</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.04215835272</span>
<span class="n">bestIteration</span> <span class="o">=</span> <span class="mi">34</span>

<span class="n">Shrink</span> <span class="n">model</span> <span class="n">to</span> <span class="n">first</span> <span class="mi">35</span> <span class="n">iterations</span><span class="o">.</span>





<span class="o">&lt;</span><span class="n">catboost</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">CatBoostClassifier</span> <span class="n">at</span> <span class="mh">0x1c1e62d588</span><span class="o">&gt;</span>
</pre></div>


<p><img src='images/newplot1.png' alt='newplot1'></p>
<p>A great feature of CatBoost is it provides information at points specified in your params (above was 'verbose = 5').</p>
<div class="highlight"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">tree_count_</span>
</pre></div>


<div class="highlight"><pre><span></span>35
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span>
<span class="k">print</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Z_val2</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[[4.01704757e-04 9.80671060e-01 1.89272355e-02]</span>
 <span class="k">[3.08763482e-04 9.96511522e-01 3.17971463e-03]</span>
 <span class="k">[3.89049775e-04 9.98622853e-01 9.88097567e-04]</span>
 <span class="na">...</span>
 <span class="k">[9.97135061e-01 2.57287349e-03 2.92065282e-04]</span>
 <span class="k">[9.96680305e-01 2.99639694e-03 3.23297602e-04]</span>
 <span class="k">[2.87351792e-04 9.95101706e-01 4.61094214e-03]]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">predictions2</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Z_val2</span><span class="p">)</span>
</pre></div>


<p>Like XGBoost, CatBoost also includes an early stopping round. In my model, I set this parameter to 10. This means that if no improvement has been made after 10 trees, the model will stop running. This helps prevent against over fitting. CatBoost also, like XGBoost, has a built in cross-validation.</p>
<div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss_function&#39;</span><span class="p">:</span> <span class="s1">&#39;MultiClass&#39;</span><span class="p">,</span>
          <span class="s1">&#39;custom_loss&#39;</span><span class="p">:</span> <span class="s1">&#39;AUC&#39;</span>
         <span class="p">}</span>

<span class="n">cross_val_results2</span> <span class="o">=</span> <span class="n">cv</span><span class="p">(</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">Pool</span><span class="p">(</span><span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">),</span>
            <span class="n">fold_count</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="c1">#number of folds to split the data into.</span>
            <span class="n">inverted</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
            <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="c1">#data is randomly shuffled before splitting.</span>
            <span class="n">partition_random_seed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">stratified</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
            <span class="n">verbose</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">cross_val_results2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>iterations</th>
      <th>test-MultiClass-mean</th>
      <th>test-MultiClass-std</th>
      <th>train-MultiClass-mean</th>
      <th>train-MultiClass-std</th>
      <th>test-AUC:class=0-mean</th>
      <th>test-AUC:class=0-std</th>
      <th>test-AUC:class=1-mean</th>
      <th>test-AUC:class=1-std</th>
      <th>test-AUC:class=2-mean</th>
      <th>test-AUC:class=2-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-1.042713</td>
      <td>0.000341</td>
      <td>-1.042747</td>
      <td>0.000093</td>
      <td>0.992025</td>
      <td>0.003681</td>
      <td>0.985580</td>
      <td>0.003899</td>
      <td>0.982720</td>
      <td>0.005111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.992463</td>
      <td>0.001261</td>
      <td>-0.992504</td>
      <td>0.001639</td>
      <td>0.997906</td>
      <td>0.001791</td>
      <td>0.992262</td>
      <td>0.002721</td>
      <td>0.984138</td>
      <td>0.005245</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.946050</td>
      <td>0.000323</td>
      <td>-0.945929</td>
      <td>0.000795</td>
      <td>0.997925</td>
      <td>0.001205</td>
      <td>0.993606</td>
      <td>0.002700</td>
      <td>0.988726</td>
      <td>0.003737</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>-0.903349</td>
      <td>0.000100</td>
      <td>-0.903072</td>
      <td>0.000699</td>
      <td>0.998220</td>
      <td>0.001502</td>
      <td>0.993857</td>
      <td>0.003076</td>
      <td>0.988547</td>
      <td>0.003624</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>-0.863164</td>
      <td>0.000607</td>
      <td>-0.863020</td>
      <td>0.000325</td>
      <td>0.998217</td>
      <td>0.001357</td>
      <td>0.994078</td>
      <td>0.002699</td>
      <td>0.988690</td>
      <td>0.003612</td>
    </tr>
  </tbody>
</table>
</div>

<p>Also similar to XGBoost, CatBoost provides the user with feature importance. By calling on 'get_feature_importance', the user can see what the model learned about the features and which of them have the greatest influence on the decisions trees.</p>
<div class="highlight"><pre><span></span><span class="n">fstrs</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="n">prettified</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">{</span><span class="n">feature_name</span> <span class="p">:</span> <span class="n">value</span> <span class="k">for</span>  <span class="n">feature_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">fstrs</span><span class="p">}</span>
</pre></div>


<div class="highlight"><pre><span></span>{&#39;redshift&#39;: 78.69840467429411,
 &#39;i&#39;: 8.742787892612705,
 &#39;u&#39;: 3.487814704869871,
 &#39;specobjid&#39;: 2.0436652601559193,
 &#39;plate&#39;: 1.8955707345778037,
 &#39;z&#39;: 1.42498154722369,
 &#39;field&#39;: 0.9011905403991105,
 &#39;run&#39;: 0.7856686087514119,
 &#39;fiberid&#39;: 0.48419622387041755,
 &#39;dec&#39;: 0.40877000998928603,
 &#39;g&#39;: 0.3113040809109803,
 &#39;ra&#39;: 0.3112717293491661,
 &#39;r&#39;: 0.25950045766926433,
 &#39;camcol&#39;: 0.24487353532627085}
</pre></div>


<p>While XGBoost had the ability to use matplotlib to plot, Catboost's equivalent is brought to us by shap.</p>
<div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">Pool</span><span class="p">(</span><span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/catboost/core.py:1697: UserWarning: &#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead
  warnings.warn(&quot;&#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead&quot;)
The model has complex ctrs, so the SHAP values will be calculated approximately.
</pre></div>


<div class="highlight"><pre><span></span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span>
</pre></div>


<p>[0.10654885020023455, 1.5818176696303439, -1.6883665198535525]</p>
<div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">Z_train2</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="images/blog_3_48_0.png"></p>
<p>Above we've done a basic model, and we've discovered that redshift is the value that has the greatest effect on the decision trees. One of the best features about CatBoost is that we can now fine-tune parameters in order to speed up the model.</p>
<div class="highlight"><pre><span></span><span class="n">fast_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">random_seed</span><span class="o">=</span><span class="mi">62</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="c1">#The default is 1000 trees. Lowering the number of trees can speed up the model. </span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="c1">#Typically, if you lower the number of trees, you should raise the learning rate.</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;Plain&#39;</span><span class="p">,</span> <span class="c1">#The default is Ordered. This prevents overfitting but is expensive in computation.</span>
    <span class="n">bootstrap_type</span><span class="o">=</span><span class="s1">&#39;Bernoulli&#39;</span><span class="p">,</span> <span class="c1">#The default is Bayesian. Bernoulli is faster.</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="c1">#less than 1 is optimal for speed.</span>
    <span class="n">rsm</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="c1">#random subspace method speeds up the training</span>
    <span class="n">leaf_estimation_iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="c1">#for smaller datasets this should be set at 1 or 5.</span>
    <span class="n">max_ctr_complexity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#We could also set one_hot_max_size to larger than the default 2. The default 2 means that any category that has 2 </span>
<span class="c1">#or less values will be hot encoded. The rest will be computated by catboosts algorithm. This is more computationally</span>
<span class="c1">#expensive.</span>
<span class="n">fast_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c1e615080&gt;
</pre></div>


<p><img src='images/newplot2.png' alt='newplot2'></p>
<p>In addition to the above parameters that were adjusted to speed up the model's performance, there are also more parameters that can be used to fine-tune the model.</p>
<div class="highlight"><pre><span></span><span class="n">tuned_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="c1"># how deep trees will go. Usually best between 1 and 10</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
    <span class="n">l2_leaf_reg</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1">#regularizer value for Ridge regression</span>
    <span class="n">bagging_temperature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">one_hot_max_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1">#2 is the default value. </span>
    <span class="n">leaf_estimation_method</span><span class="o">=</span><span class="s1">&#39;Newton&#39;</span><span class="p">,</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s1">&#39;MultiClass&#39;</span>
<span class="p">)</span>
<span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">(</span><span class="n">Z_val2</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
    <span class="n">plot</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c400a10b8&gt;
</pre></div>


<p><img src='images/newplot3.png' alt='newplot3'></p>
<div class="highlight"><pre><span></span><span class="n">final_model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">random_seed</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">tuned_model</span><span class="o">.</span><span class="n">tree_count_</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">final_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Z_train2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Learning rate set to 0.055965
0:  learn: 0.5431275    total: 61.7ms   remaining: 30.8s
100:    learn: 0.0057702    total: 6.14s    remaining: 24.3s
200:    learn: 0.0032198    total: 11.3s    remaining: 16.8s
300:    learn: 0.0021840    total: 16.3s    remaining: 10.8s
400:    learn: 0.0016674    total: 21.1s    remaining: 5.21s
499:    learn: 0.0013561    total: 25.5s    remaining: 0us





&lt;catboost.core.CatBoostClassifier at 0x1c1e2a59e8&gt;
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">get_best_score</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>{&#39;learn&#39;: {&#39;Logloss&#39;: 0.0013560782767138484}}
</pre></div>


<p>The greatest benefit of CatBoost is its ability to handle categorical features. With other boosting software, we need to preprocess data on our own. This would be done using LabelBinarizer (as shown above in the XGBoost section). Although DataFrameMapper makes this task more manageable, in some cases it may lead to datasets that are unimaginably large. In addition to combination features(combining columns), which are converted to number values (these combination features are not used in the first split, but then all will be used in the next splits), CatBoost has a special formula to convert categorical features into numbers. With CatBoost and its categorical features, the data is shuffled and a mean is calculated for every object on its historical data.</p>
<p>ctr i = (countInClass + prior) / (totalCount + 1)</p>
<p>CountInClass is how many times the label value was equal to i for objects with the current categorical feature value.</p>
<p>TotalCount is the total number of objects already visited that have the same feature value as the one being visited.</p>
<p>Prior is a constant defined by the starting parameters</p>
<p>When it comes to missing values, CatBoost also deals with these during training. The default setting is 'Min' which means that missing values are processed as the minimum value for the feature. It is also guaranteed that a split that seperates all missing values from all other values is considered when selecting trees.</p>
<h1>Conclusion</h1>
<p>In addition to categorical features, another main diference between XGBoost and CatBoost is the type of decision trees used. XGBoost uses assymetric trees (which could be viewed above) while CatBoost uses oblivious trees as base predictors. An assymetric tree makes splits that will learn the most. Oblivious trees simply means that the feature used for splitting is the same accross all intermediate nodes within the same level of the tree.</p>
<p>Overall, I find it's hard to choose the best boosting system, because both do a great job and offer great features to work with. This can be viewed by their use in winning Kaggle competition entries. However, I find CatBoost's ability to pass categorical features, instead of one-hot-encoding, novel. It saves time in preparing the data to be modeled and although more computationally expensive at first, I think with larger datasets, where one-hot-encoding could swell the dataset to enormous sizes, it would be more effective. I like the fact that it provides both options when you're fitting a model. The default is 2 - which means that if a categorical column has two or less categories it will hot encode, and if its more than 2, it will run its built-in calculation for categorical columns. Either way, categorical columns are processed internally when training the model. Thus, even if you prefer categorical columns to be one-hot-encoded, you can set the parameters accordingly and not have to pre-process these columns. </p>
<p>It's hard to choose between the two libraries because I think they both excel with larger datasets than the one I used. However, with my computer and the SkyServer dataset, CatBoost outperformed XGBoost in speed, and I found the ability to fine-tune and speed-up the model more user-friendly and intuitive. Also I found the instant plotting more engaging, and the shap add-on, more visually appealing.</p>

            </article>
        </section>
    </div> <!-- span -->
</div>                        </div>
                    </div> <!-- content row -->

                    <div class="row"> <!-- footer -->
                        <div class="span7 offset2">
                            <footer id="site-footer">
                                <p>
                                    <a href="" target="_blank">
                                        
                                    </a> Kristi Gourlay /
                                    <a href="https://KristiGourlay.github.io/blog/">Archives</a>
                                </p>
                                <p>
                                    Powered by
                                    <a href="https://github.com/getpelican/pelican" target="_blank">
                                        Pelican
                                    </a>
                                    /
                                    Source code on
                                    <a href="http://github.com/siovene/iovene.com" target="blank">
                                        GitHub
                                    </a>
                                    /
                                    Theme <code>Lannisport</code>, also on
                                    <a href="http://github.com/siovene/lannisport" target="_blank">
                                        GitHub
                                    </a>
                                </p>
                            </footer>
                        </div> <!-- footer span -->
                    </div> <!-- footer row -->
                </div> <!-- header+content+footer span -->

                <div class="span3">
                    <aside class="affix">
                        <div id="sidebar">
                            <div id="social-icons">
                            </div>

                            <div id="navigation">
<nav>
	<h2></h2>
	<ul>
		<li>
			<ul>
				<li>
					<a href="https://KristiGourlay.github.io/blog">Newest Blog</a>
				</li>

			</ul>
		</li>
	</ul>
</nav>


<nav>
	<h2></h2>
	<ul>
		<li>
			<ul>
					<li>
						<a href="https://KristiGourlay.github.io/blog/category/misc.html">List of All Blog Posts</a>
					</li>
			</ul>
		</li>
	</ul>
</nav>

<!--
<nav>
	<h2>Categories</h2>
	<ul>
		<li>
			<ul>
					<li>
						<a href="https://KristiGourlay.github.io/blog/category/misc.html">misc</a>
					</li>
			</ul>
		</li>
	</ul>
</nav> -->                            </div>
                        </div> <!-- sidebar -->

                        <p id="back-to-top">
                            <a href="#">
                                <i class="icon-double-angle-up"></i> Back to top
                            </a>
                        </p>

                    </aside>
                </div> <!-- logo+navigation span -->
            </div> <!-- row -->

        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>

        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/js/bootstrap.min.js"></script>
        <script>$('body').popover || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/bootstrap.min.js"><\/script>')</script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/jquery.captions.js"></script>
        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery.colorbox-min.js"></script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/main.js"></script>

        <script>
            var _gaq=[['_setAccount',''],['_trackPageview'],['_trackPageLoadTime'],['_gat._anonymizeIp']];
            (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
            g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
            s.parentNode.insertBefore(g,s)}(document,'script'));
        </script>
    </body>
</html>