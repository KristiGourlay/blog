<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Kristi's Blog</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="shortcut icon" href="https://KristiGourlay.github.io/blog/theme/favicon.ico" />
        <link rel="apple-touch-icon" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-iphone4.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://KristiGourlay.github.io/blog/theme/apple-touch-icon-ipad3.png" />

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/colorbox.css">

        <link rel="stylesheet" href="https://KristiGourlay.github.io/blog/theme/css/main.css">

        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    </head>
    <body>

        <div class="container">
            <div class="row">
                <div class="span9">
                    <div class="row">
                        <div class="span2"> <!-- logo -->
                            <div id="logo">
                                <a href="https://KristiGourlay.github.io/blog">
                                    <img alt="Kristi's Blog" src="https://KristiGourlay.github.io/blog/" />
                                </a>
                            </div> <!-- logo -->
                        </div> <!-- logo span -->

                        <div class="span7"> <!-- header -->
                            <!--[if lt IE 9]>
                                <p class="chromeframe alert alert-warning">You are using an <strong>outdated</strong> browser, and this site might not look best in it. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
                            <![endif]-->
                            <header id="site-header">
                                <div id="site-header-content">
                                    <h1>
                                        <a href="https://KristiGourlay.github.io/blog">Kristi's Blog</a>
                                        <small>
                                            <span class="divider">/</span> 
                                        </small>
                                    </h1>
                                    <p>
                                        
                                    </p>
                                </div>
                            </header>
                        </div> <!-- header span -->
                    </div> <!-- header row -->

                    <div class="row"> <!-- content -->
                        <div class="content">
    <div id="index">
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_4.html" rel="bookmark">You Pro-BAYE-bly will FREQUENT this Post Before an Interview</a></h2>
            <time datetime="2019-05-08T10:20:00-04:00">
                May 08, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div id="first-article">
        <p>Most Data Science questions deal with studying populations. A population is a set of similar items or events that are of interest for a question or an experiment. Since the task of measuring  an entire population is frequently too expensive and impractical, we take samples, and make inferences about the whole population based on the statistics we find in the sample. In statistical inference we have four key concepts:</p>
<div class="highlight"><pre><span></span>            -Samples
            -Statistics
            -Parameters
            -Populations
</pre></div>


<p>Generally speaking, statistics describe samples and parameters describe populations. Statistical inference is how we move from statistics taken on a sample, to parameters about the whole population. For Frequentists the two main ways we can generalize from a sample to a populations are through Confidence Intervals and Hypothesis Tests. For Bayesians, probability is assigned to a hypothesis, where a prior probability is updated  to a posterior probability, with relevant data. This blog post will outline and compare these two theories in light of Data Science.</p>
<h1>Frequentist Probability</h1>
<p>Frequentists have three mathematical concepts that their experiments rely on - the null hypothesis, the associated p value, and the confidence interval. Frequentists have a null hypothesis (known as H0) and an alternate hypothesis (H1). An example of this is if a scientist was running a drug trial where he split the subjects into two groups: those administered the drug and those administered a placebo. The null hypothesis would be the assumption that there was no difference between the two groups. And this would be the stance of the scientist. After this, the scientist would create a reasonable threshold for rejecting the null hypothesis. This notation - α (alpha) - a real number between 0 and 1 - is known as the p value. The p value is the probability of observing a deviation from the null hypothesis which is greater than or equal to the one you observed. Another way of thinking about it, is that the p value is the probability of the data, given that the null hypothesis is true. The scientist will reject the null hypothesis if the p value is below α and not reject otherwise. Alpha is generally set to 0.05. This is considered standard practice.</p>
<p>The best way to explain p value and the null hypothesis is by a drug efficacy example. Let's say that we've just created a new sleeping pill. We've randomly selected 100 people who have problems sleeping; 50 will be administered the sleeping pill, and 50 will be given a placebo. The group given the sleeping pill is the "experiment" group and the group given the placebo is known as the "control" group. In this situation the null hypothesis will be that there will be no difference in the sleeping patterns - hours slept - between the two groups. The alternate hypothesis is that there will be a difference in hours slept between the two groups. The p value (the level of significance) will be .05. Thus, we will reject the null hypothesis if the p value is below .05.</p>
<p>Let's simulate two groups - control and experiment - and their average hours slept per night during the trial.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">control</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">control</span><span class="p">)</span>
<span class="n">experiment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">experiment</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[8 5 7 7 4 5 6 2 6 3 3 8 6 6 7 5 8 3 6 7 4 2 8 3 3 4 9 4 5 3 8 8 5 2 9 3 8
 8 9 2 9 8 7 9 2 3 8 9 8 5]
[6 4 9 9 6 9 8 5 8 4 7 9 7 9 9 5 5 5 4 6 7 6 9 7 6 8 8 5 8 4 4 8 5 7 4 9 5
 5 9 6 5 7 4 8 8 4 9 5 5 7]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">control</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">experiment</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">experiment</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">control</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>5.74
6.52
0.7799999999999994
</pre></div>


<p>The measure of difference is 0.78. What is the probability that we observed this data, assuming that the null hypothesis is true?</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">stats</span>

<span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">control</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Ttest_indResult(statistic=1.8573147670624264, pvalue=0.06626937134706777)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">control</span><span class="p">)</span>
</pre></div>


<p>The p value is .066, thus we must accept the null hypothesis and state that there is no difference between the sleeping patterns - hours slept - between the experiment and control groups.</p>
<p>The third important key in Frequentist statisitics is the confidence interval. Simply put, a confidence interval is a range of values, in which the actual value is likely to fall. It represents the accuracy or precision of a given estimate. A confidence interval can be calculated as so: </p>
<p>$$
\text{[sample statistic]} \pm \text{[multiplier]} \times \text{[standard deviation of sampling distribution]}
$$</p>
<p>Let's simulate a population and a poll as an example. Say we live in a state and we are working for a newspaper and we are trying to determine whether a certain proposition will pass. Unfortunately, we dont have the time, or the money, to run a poll that will reach out and question all 1 million voters. If we were to run an experiment, we would poll a sample of the society, and then use those samples to make inferences about the whole population. This is how a frequentist would tackle this situation. First I will generate statistics about the state.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span> <span class="mf">0.60</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">1</span><span class="n">_000_000</span><span class="p">)</span> <span class="c1">#similating a population where 60% would vote yes.</span>
<span class="n">population</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([1, 0, 0, 1, 1, 1, 1, 0, 0, 0])
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.554
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>0.614
</pre></div>


<p>In this scenario, the population is 1,000,000, the sample is 500 people, the statistics would be the percentage of people in the sample voting yes on the proposition, and the parameter is the true percentage of people voting yes on the proposition.</p>
<p>We at the newspaper, do not know that 60% of the population would vote for this proposition to pass, but by looking at a sample of 500 voters, we can make educated inferences about how the entire population may vote. This is the Central Limit Theorem in action. This is a probability theory that states that when independent random variables are added their sum tends toward a normal distribution, even if the original variables are not normally distributed. It is this theory that allows for conclusions to be made about the whole population from a sample of the population. Of the sample of the first 500 citizens who were polled, 55.4% responded that they would vote for the proposition to pass. A frequentist would then calculate the margin or error, creating the confidence interval by calculating the mean and the standard deviation and combining this with the associated Z score (1.96). The area under the standard normal distribution between -1.96 and +1.96 is 95%. The confidence interval is a set of likely values for the parameter of interest. </p>
<p><img src='images/1.96.jpg' alt='1.96'></p>
<div class="highlight"><pre><span></span><span class="n">sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">sample_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">lower</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">sample_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">higher</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">sample_mean</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">f</span><span class="s1">&#39;I am 95</span><span class="si">% c</span><span class="s1">onfident that the true population percentage of people who will vote yes on this proposition is between {lower} and {higher}&#39;</span>
</pre></div>


<div class="highlight"><pre><span></span>&#39;I am 95% confident that the true population percentage of people who will vote yes on this proposition is between 0.5104 and 0.5976&#39;
</pre></div>


<p>'I am 95% confident that the true population percentage of people who will vote yes on this proposition is between 0.5104 and 0.5976'</p>
<p>Frequentist statistics allow us to make inferences about the greater population by using a smaller sample of the population and the statistics observed from that sample. The main critiques of Frequentist theory however, is that the p value and the confidence interval depend on the sample size, as Frequentist theory does not perform well on sparse data sets. Moreover, the confidence intervals are not probability distributions. These two flaws in Frequentist theory are remedied in Bayesian Statistics. </p>
<h1>BAYESIAN PROBABILITY</h1>
<p>Bayesian statistics successfully apply probabilities to statistical problems. In addition to this, it provides the ability to upgrade probabilities with the introduction of new data. It includes three components:</p>
<div class="highlight"><pre><span></span>            -The Prior
            -The Likelihood
            -The Posterior
</pre></div>


<p>The Prior is our belief about paramenters based on previous experience. This takes into account both previous data compiled, in addition to, our own beliefs based on experience.
The Likelihood is the specific observed data.
The Posterior distribution combines the prior and the likelihood. It is the multiplication of the Likelihood and the Prior.
The Posterior distribution is calculated by the following:</p>
<p>$$
\begin{eqnarray<em>}
\text{posterior} &amp;\propto&amp; \text{prior } \times \text{likelihood} \
&amp;&amp; \
f(p|\text{data}) &amp;\propto&amp; f(p) \times f(\text{data}|p)
\end{eqnarray</em>}
$$</p>
<p>For the sake of comparison with Frequentist theory, you could say that the Bayes factor is the equivalent of the p value. The Bayes factor is the ratio of the probability of one hypothesis in relation to the probability of another hypothesis. So for instance, in our sleeping pill example, the ratio of the probability of improved sleeping in the control group vs. that of the experiment group.</p>
<p>The High Density Interval (also known as the Credibility Interval) is the Bayesian equivalent to the Confidence Interval. The Credibility Interval is formed by the posterior distribution. While the Confidence Interval is a collection of intervals with 95% of them containing the true parameter, the Credibility Interval provides an interval that has a 95% chance of containing the true parameter. The Credibility Interval is independent of intentions and sample size. This is good for Data Science, because if we have few data points, we can use strong priors.</p>
<p>Now let's look at the same polling example from above performed with Bayesian inference. First we will create our prior. In this case, let's pretend that based on other statistics we've seen, and previous voting habits in this state, that we have a very strong prior belief that 20% of the state will vote yes on the proposition. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>


<span class="n">alpha_prior</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="mi">800</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">alpha_prior</span><span class="p">,</span> <span class="n">beta_prior</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prior Belief&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Values of P&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>


<p><img alt="png" src="images/blog_4_27_0.png"></p>
<p>Now we will incorporate our Likelihood and calculate the Posterior based on the Prior and the Likelihood. The Likelihood, calculated by n_trials and n_successes, will be equal to the data that we "collected" above in the Frequentist example.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">800</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">n_successes</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1">#copying the data from the sample used above</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">n_successes</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">n_successes</span> <span class="o">/</span> <span class="n">n_trials</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">n_successes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)],</span>
               <span class="n">ymin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
               <span class="n">ymax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">prior</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">)),</span> 
               <span class="n">linestyles</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
               <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior, Likelihood, and Posterior Modes&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Values of P&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>   
</pre></div>


<p><img alt="png" src="images/blog_4_29_0.png"></p>
<p>As you can see in the graph above, we have set our priors (a= 200, b= 800) very strong, so that even though the actual data collected was at around 60% voting yes on the proposition, the strength of our priors, has pulled the posterior mean to around 35%. 
If our prior beliefs were not strong, then we would set our alpha and beta much lower. In the following model, I will set them at 2 and 8 to show the effect.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">n_successes</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1">#copying the data from the sample used above</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">n_successes</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">n_successes</span> <span class="o">/</span> <span class="n">n_trials</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">n_successes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)],</span>
               <span class="n">ymin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
               <span class="n">ymax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">prior</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">)),</span> 
               <span class="n">linestyles</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
               <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior, Likelihood, and Posterior Modes&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Values of P&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>   
</pre></div>


<p><img alt="png" src="images/blog_4_31_0.png"></p>
<p>As you can see, with a weaker prior, the posterior is pulled to the left toward the prior, but only slightly. The posterior is being dominated by the data. A perfect example of when you would set a weak prior would be the first example above of a sleeping pill clinical trial. This makes the point that in Bayesian statistics, a prior is not necessary to draw a valuable conclusion (Although, we could specify a weak prior based on expert analysis in the field). In the sleeping pill example, the p value was 0.6, and thus this was interpreted as being insufficient evidence that the sleeping pill worked. There was clearly a difference observed between the two groups, but it was not judged to be sufficient enough of a difference in the standard Frequentist approach. Instead of testing whether two groups are different, Bayesians attempt to measure the difference between the two groups. A topic for a later blog!!!</p>
<p>CONCLUSION</p>
<p>While Frequentists would say that Bayesians incorporating prior beliefs is wrong, the basic idea of not including your prior beliefs, is already making a judgment call about the world. Including priors, results in more accurate distributions, because you're not including the probabilities for data points, that you know from common sense, are not possible. Furthermore, including priors allows us to incorporate expert opinions and specific knowledge into our models. When our model makes a prediction, it provides distribution of likely answers. These predictions are highly interpretable and easier to understand than p values and Confidence Intervals. Finally, if we do not have strong prior beliefs about certain areas, we can always set our prior beleifs to be weak. Thus, our posterior will be dominated by our data, and not our prior.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_3.html" rel="bookmark">Kitty Got Claws! A Comparison of CatBoost and XGBoost</a></h2>
            <time datetime="2019-04-23T10:20:00-04:00">
                Apr 23, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>Boosting is an ensemble technique where new models are created to correct the errors of past models. Each subsequent tree learns from the errors of its predecessor, as all trees are added sequentially until no further improvements can be made. Gradient boosting is an approach where new models are created from the residuals or errors of prior models and then are added together to make the final prediction. The base learners in gradient boosting are weak learners, but each contributes vital information for the final model. Two examples of these programs are XGBoost and CatBoost. They are two open-source software libraries that provide a gradient boosting framework. </p>
<p>To compare these two programs, I will load a dataset called "Skyserver", and I will attempt to predict whether an image is a Star, Galaxy, or Quasar. I will begin with XGBoost.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="kp">loadtxt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span><span class="p">,</span> <span class="n">FunctionTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">import</span> <span class="nn">catboost</span> <span class="kn">as</span> <span class="nn">cb</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">Pool</span>
<span class="kn">import</span> <span class="nn">shap</span>

<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_tree</span><span class="p">,</span> <span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">plot_importance</span>


<span class="n">sky</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Skyserver.csv&#39;</span><span class="p">)</span>
</pre></div>


<p>The first thing I do with a dataset is some basic EDA. This includes checking to see whether there are null values: sky.isnull().sum(). The Skyserver dataset is very clean and there are no null values. However, with XGBoost, even if there were null values, XGBoost has a built in algorithm to impute these missing values. Its algorithm automatically learns what is the best imputation value for missing values based on training. This is one key advantage of using XGBoost. </p>
<p>Before I am ready to model, I will prepare my dataset, by labeling the 3 items in the target column - 'star', 'galaxy', 'qso'. Then I will drop unnecessary columns, and assign my y(target) and X(features).</p>
<div class="highlight"><pre><span></span>sky[&#39;class&#39;] = sky[&#39;class&#39;].map({&#39;STAR&#39;: 0, &#39;GALAXY&#39;: 1, &#39;QSO&#39;: 2})
</pre></div>


<div class="highlight"><pre><span></span>sky = sky.drop(columns=[&#39;objid&#39;, &#39;rerun&#39;, &#39;mjd&#39;]) 
#drop &#39;objid&#39; because irrelevant to model and &#39;rerun&#39; because it has the same value for each row.
</pre></div>


<div class="highlight"><pre><span></span>y = sky[&#39;class&#39;]
X = sky.drop(&#39;class&#39;, axis=1)
</pre></div>


<div class="highlight"><pre><span></span>data_dmatrix = xgb.DMatrix(data=X, label=y) 
#DMatrix is an internal data structure used in XGBoost. It is recommended as it is optimal for memory and speed.
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, &#39;base&#39;, None) is not None and \
/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version
  data.base is not None and isinstance(data, np.ndarray) \
</pre></div>


<p>Having completed basic EDA, assigned our X and y, and created a data dmatrix, we are ready to split out data into training and testing sets and use DataFrameMapper, StandardScaler and LabelBinarizer to prepare our data. We do not need to worry about imputing at this point, because as stated above, in addition to the dataset being clean of missing values, XGBoost has a built in imputer that deals with missing values.</p>
<div class="highlight"><pre><span></span>X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
</pre></div>


<p>Obviously a key difference between XGBoost and CatBoost is how each deals with categorical columns. I'll discuss CatBoost later, but for now, as is shown below, for XGBoost, categorical columns need to be one hot encoded, using LabelBinarizer. Knowing that creating dummy columns, in many instances, could expand datasets to unimaginable sizes, the XGBoost developers introduced a built-in method to deal with this, called Automatic Sparse Data Optimization. The developers chose to handle this in the same way that they handled the existence of missing values, by making the algorithm aware of the sparsity pattern in the dataset. By only visiting the present values, and not the missing values and zeroes; it exploits the sparsity and learns the best directions to handle missing values. It handles all cases of sparsity in a unified way. </p>
<div class="highlight"><pre><span></span><span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
    <span class="p">([</span><span class="s1">&#39;ra&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;dec&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;u&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;run&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;camcol&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;specobjid&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">([</span><span class="s1">&#39;redshift&#39;</span><span class="p">],</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;plate&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;fiberid&#39;</span><span class="p">,</span> <span class="n">LabelBinarizer</span><span class="p">()),</span>
<span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df_out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Z_train</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_val</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">DataConversionWarning</span><span class="p">)</span>
</pre></div>


<p>Now it's time to model, predict, and score!</p>
<div class="highlight"><pre><span></span>model1 = xgb.XGBClassifier(
    n_estimators = 500, #How many trees.
    num_class = 3, #Identifying there are 3 classes in our target
    objective = &#39;reg:logistic&#39;, #Objective refers to which model you want to use.
    eval_set=&#39;Accuracy&#39;, #Eval_set refers to how you want your model to be evaluated.
    learning_rate=0.1, #Learning_rate has to do with how much you want each tree to learn.
    early_stopping_rounds=10, #stop after ten iterations if no improvements have been made.            
    max_depth=3, #Smaller trees are more interpretable
    silent=False) #Silent deals with whether we want information for each iteration to be printed out.
</pre></div>


<div class="highlight"><pre><span></span>model1.fit(Z_train, y_train)
</pre></div>


<div class="highlight"><pre><span></span>XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
       colsample_bytree=1, early_stopping_rounds=10, eval_set=&#39;Accuracy&#39;,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
       min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,
       nthread=None, num_class=3, objective=&#39;multi:softprob&#39;,
       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
       seed=None, silent=False, subsample=1)
</pre></div>


<div class="highlight"><pre><span></span>preds = model1.predict(Z_val)
</pre></div>


<div class="highlight"><pre><span></span>mean_squared_error(y_val, preds)
</pre></div>


<div class="highlight"><pre><span></span>0.01
</pre></div>


<p>Another key feature is that there is a built-in cross validation. This runs a cross validation at each iteration of the boosting process.</p>
<div class="highlight"><pre><span></span>params = {&#39;colsample_bytree&#39;: &#39;0.3&#39;, #the fraction of columns to be randomly sampled for each tree.
        &#39;learning_rate&#39;: &#39;0.2&#39;, 
        &#39;max_depth&#39;: &#39;5&#39;, #the maximum depth of a tree
        &#39;alpha&#39;: &#39;10&#39;} #L1 regularization weight - Lasso

cross_val_results = xgb.cv(dtrain=data_dmatrix, params=params) #this is where the dmatrix comes in handy.
</pre></div>


<div class="highlight"><pre><span></span>cross_val_results.head()
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train-rmse-mean</th>
      <th>train-rmse-std</th>
      <th>test-rmse-mean</th>
      <th>test-rmse-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.615842</td>
      <td>0.001887</td>
      <td>0.617878</td>
      <td>0.001618</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.514400</td>
      <td>0.023372</td>
      <td>0.517281</td>
      <td>0.024252</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.456278</td>
      <td>0.013461</td>
      <td>0.460037</td>
      <td>0.015424</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.420923</td>
      <td>0.006550</td>
      <td>0.426109</td>
      <td>0.009830</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.385381</td>
      <td>0.030245</td>
      <td>0.391525</td>
      <td>0.033427</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>print((cross_val_results[&quot;test-rmse-mean&quot;]).tail(1))
</pre></div>


<div class="highlight"><pre><span></span>9    0.281167
Name: test-rmse-mean, dtype: float64
</pre></div>


<div class="highlight"><pre><span></span>xgreg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=15)
</pre></div>


<p>A benefit of using boosting is that it provides you with feature importance. It provides you with a score which indicates how valuable a feature was in the construction of the boosted decision tree. The more a certain feature is used to make key decisions in the decision tree, the higher its importance. Importance is calculated by the the amount that each split point improves the overall tree, weighted over the number of observations it is resposible for.</p>
<div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(18, 18))
plot_importance(model1, ax=ax)
plt.show()
</pre></div>


<p><img alt="png" src="images/blog_3_23_0.png"></p>
<p>As can be viewed by the chart, Redshift is by the far the most important feature.</p>
<p>XGBoost also provides the ability to peak into your model and inspect a tree. </p>
<div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(20, 20))
plot_tree(xgreg, num_trees=4, ax=ax)
plt.show()
</pre></div>


<p><img alt="png" src="images/blog_3_25_0.png"></p>
<p>Above you can see how a decision tree on this data looks. You can see that it is assymetrical, which is an important point of comparison with CatBoost's trees. One interesting aspect about this tree is the fact that you can see that the algorithm has chosen to impute all 'missing' values with the direction of yes. </p>
<p>According to XGBoost's documentation, website, and papers given by founder, Tianqi Chen, in addition to the features described above, XGBoost's power also comes from its 'Block structures for parallel learning', 'Cache Awareness', and 'Out of Core Computing'. Data is sorted and stored in memory units called blocks. This allows the data to easily be re-used in later iterations, instead of the data having to be computed again. The data in each block is stored in a compressed column format, and with one scan of the block, the statistics for the split candidates can be found. Cache awareness is the existence of internal buffers where gradient stats can be stored. Out of core computing allows optimized disk space and maximized usage when dealing with datasets that are too large to fit in memory.</p>
<p>The one area where XGBoost is lacking is in feature engineering and hyper-parameter tuning. Chen, and the XGBoost team however, believe this is fine, as these areas are deeply integrated within coding programs. After reading documentation and several data science blogs on boosting systems, I see that there is an ability to grid search to find the best parameters. However, considering these boosting frameworks are designed for extremely large datasets, if we were to grid search one of these files, it would be computationally expensive, and time consuming. (Running a basic cross-validation ran almost 15 minutes). In order to improve a model, hyper-tuning is necessary, but it's hard to determine which parameters should be tuned and what the ideal value of each should be. </p>
<h1>Catboost</h1>
<p>CatBoost is another open-source gradient boosting on decision trees library. CatBoost's claim to fame is its categorical feature support. Instead of having to take care of one-hot-encoding during pre-processing the data, the CatBoost algorithm handles categorical columns while training the data. </p>
<p>I'm now going to use DataFrameMapper and StandardScaler to prepare the rest of the data. Following this, I will pass the categorical features, while fitting the model.</p>
<div class="highlight"><pre><span></span>mapper2 = DataFrameMapper([
    ([&#39;ra&#39;], StandardScaler()),
    ([&#39;dec&#39;], StandardScaler()),
    ([&#39;u&#39;], StandardScaler()),
    ([&#39;g&#39;], StandardScaler()),
    ([&#39;r&#39;], StandardScaler()),
    ([&#39;i&#39;], StandardScaler()),
    ([&#39;z&#39;], StandardScaler()),
    ([&#39;run&#39;], StandardScaler()),
    ([&#39;specobjid&#39;], StandardScaler()),
    ([&#39;redshift&#39;], StandardScaler()),
], default=None, df_out=True)

Z_train2 = mapper2.fit_transform(X_train)
Z_val2 = mapper2.transform(X_val)
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)
</pre></div>


<div class="highlight"><pre><span></span># categorical_features = list(range(0, X.shape[1]))
categorical_features = np.where(Z_train2.dtypes == int)[0]
</pre></div>


<div class="highlight"><pre><span></span>print(categorical_features)
</pre></div>


<div class="highlight"><pre><span></span>[10 11 12 13]
</pre></div>


<div class="highlight"><pre><span></span>model2 = CatBoostClassifier(
    iterations=100,
    learning_rate=0.5,
    early_stopping_rounds = 10,
    loss_function = &#39;MultiClass&#39;, #For 2-class classification, we should use &quot;Logloss&quot; or &quot;Entropy&quot;
    custom_metric=&#39;AUC&#39;,
    use_best_model=True,
    verbose=5 #information is printed out at every fifth iteration
)
model2.fit(
    Z_train2, y_train,
    cat_features=categorical_features,
    eval_set=(Z_val2, y_val),
    plot=False #This should be True. But the printout is not compatible with this blog framework
)
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4231271</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.4268821</span> <span class="o">(</span><span class="mi">0</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">108</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">10.7</span><span class="n">s</span>
<span class="mi">5</span><span class="o">:</span>  <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0695255</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0717988</span> <span class="o">(</span><span class="mi">5</span><span class="o">)</span>    <span class="n">total</span><span class="o">:</span> <span class="mi">447</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mi">7</span><span class="n">s</span>
<span class="mi">10</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461552</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0538007</span> <span class="o">(</span><span class="mi">10</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mi">947</span><span class="n">ms</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.67</span><span class="n">s</span>
<span class="mi">15</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0407958</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0510322</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0508512</span> <span class="o">(</span><span class="mi">14</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.46</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.68</span><span class="n">s</span>
<span class="mi">20</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0350664</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0461989</span> <span class="o">(</span><span class="mi">20</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">1.99</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.48</span><span class="n">s</span>
<span class="mi">25</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0315475</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0459406</span> <span class="o">(</span><span class="mi">25</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.47</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">7.03</span><span class="n">s</span>
<span class="mi">30</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0272609</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0449078</span> <span class="o">(</span><span class="mi">30</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">2.96</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.6</span><span class="n">s</span>
<span class="mi">35</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0247762</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422198</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">3.52</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">6.26</span><span class="n">s</span>
<span class="mi">40</span><span class="o">:</span> <span class="n">learn</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0233293</span>   <span class="n">test</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0422688</span>    <span class="n">best</span><span class="o">:</span> <span class="o">-</span><span class="mf">0.0421584</span> <span class="o">(</span><span class="mi">34</span><span class="o">)</span>   <span class="n">total</span><span class="o">:</span> <span class="mf">4.09</span><span class="n">s</span>    <span class="n">remaining</span><span class="o">:</span> <span class="mf">5.88</span><span class="n">s</span>
<span class="n">Stopped</span> <span class="n">by</span> <span class="n">overfitting</span> <span class="n">detector</span>  <span class="o">(</span><span class="mi">10</span> <span class="n">iterations</span> <span class="n">wait</span><span class="o">)</span>

<span class="n">bestTest</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.04215835272</span>
<span class="n">bestIteration</span> <span class="o">=</span> <span class="mi">34</span>

<span class="n">Shrink</span> <span class="n">model</span> <span class="n">to</span> <span class="n">first</span> <span class="mi">35</span> <span class="n">iterations</span><span class="o">.</span>





<span class="o">&lt;</span><span class="n">catboost</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">CatBoostClassifier</span> <span class="n">at</span> <span class="mh">0x1c1e62d588</span><span class="o">&gt;</span>
</pre></div>


<p><img src='images/newplot1.png' alt='newplot1'></p>
<p>A great feature of CatBoost is it provides information at points specified in your params (above was 'verbose = 5').</p>
<div class="highlight"><pre><span></span>model2.tree_count_
</pre></div>


<div class="highlight"><pre><span></span>35
</pre></div>


<div class="highlight"><pre><span></span>model2.predict_proba
print(model2.predict_proba(data=Z_val2))
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[[4.01704757e-04 9.80671060e-01 1.89272355e-02]</span>
 <span class="k">[3.08763482e-04 9.96511522e-01 3.17971463e-03]</span>
 <span class="k">[3.89049775e-04 9.98622853e-01 9.88097567e-04]</span>
 <span class="na">...</span>
 <span class="k">[9.97135061e-01 2.57287349e-03 2.92065282e-04]</span>
 <span class="k">[9.96680305e-01 2.99639694e-03 3.23297602e-04]</span>
 <span class="k">[2.87351792e-04 9.95101706e-01 4.61094214e-03]]</span>
</pre></div>


<div class="highlight"><pre><span></span>predictions2 = model2.predict(data=Z_val2)
</pre></div>


<p>Like XGBoost, CatBoost also includes an early stopping round. In my model, I set this parameter to 10. This means that if no improvement has been made after 10 trees, the model will stop running. This helps prevent against over fitting. CatBoost also, like XGBoost, has a built in cross-validation.</p>
<div class="highlight"><pre><span></span>params = {&#39;loss_function&#39;: &#39;MultiClass&#39;,
          &#39;custom_loss&#39;: &#39;AUC&#39;
         }

cross_val_results2 = cv(
            params = params,
            pool = Pool(Z_train2, y_train, cat_features=categorical_features),
            fold_count = 5, #number of folds to split the data into.
            inverted = False,
            shuffle = True, #data is randomly shuffled before splitting.
            partition_random_seed = 0,
            stratified = False,
            verbose = False)
</pre></div>


<div class="highlight"><pre><span></span>cross_val_results2.head()
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>iterations</th>
      <th>test-MultiClass-mean</th>
      <th>test-MultiClass-std</th>
      <th>train-MultiClass-mean</th>
      <th>train-MultiClass-std</th>
      <th>test-AUC:class=0-mean</th>
      <th>test-AUC:class=0-std</th>
      <th>test-AUC:class=1-mean</th>
      <th>test-AUC:class=1-std</th>
      <th>test-AUC:class=2-mean</th>
      <th>test-AUC:class=2-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-1.042713</td>
      <td>0.000341</td>
      <td>-1.042747</td>
      <td>0.000093</td>
      <td>0.992025</td>
      <td>0.003681</td>
      <td>0.985580</td>
      <td>0.003899</td>
      <td>0.982720</td>
      <td>0.005111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.992463</td>
      <td>0.001261</td>
      <td>-0.992504</td>
      <td>0.001639</td>
      <td>0.997906</td>
      <td>0.001791</td>
      <td>0.992262</td>
      <td>0.002721</td>
      <td>0.984138</td>
      <td>0.005245</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.946050</td>
      <td>0.000323</td>
      <td>-0.945929</td>
      <td>0.000795</td>
      <td>0.997925</td>
      <td>0.001205</td>
      <td>0.993606</td>
      <td>0.002700</td>
      <td>0.988726</td>
      <td>0.003737</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>-0.903349</td>
      <td>0.000100</td>
      <td>-0.903072</td>
      <td>0.000699</td>
      <td>0.998220</td>
      <td>0.001502</td>
      <td>0.993857</td>
      <td>0.003076</td>
      <td>0.988547</td>
      <td>0.003624</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>-0.863164</td>
      <td>0.000607</td>
      <td>-0.863020</td>
      <td>0.000325</td>
      <td>0.998217</td>
      <td>0.001357</td>
      <td>0.994078</td>
      <td>0.002699</td>
      <td>0.988690</td>
      <td>0.003612</td>
    </tr>
  </tbody>
</table>
</div>

<p>Also similar to XGBoost, CatBoost provides the user with feature importance. By calling on 'get_feature_importance', the user can see what the model learned about the features and which of them have the greatest influence on the decisions trees.</p>
<div class="highlight"><pre><span></span>fstrs = model2.get_feature_importance(prettified=True)
</pre></div>


<div class="highlight"><pre><span></span>{feature_name : value for  feature_name, value in fstrs}
</pre></div>


<div class="highlight"><pre><span></span>{&#39;redshift&#39;: 78.69840467429411,
 &#39;i&#39;: 8.742787892612705,
 &#39;u&#39;: 3.487814704869871,
 &#39;specobjid&#39;: 2.0436652601559193,
 &#39;plate&#39;: 1.8955707345778037,
 &#39;z&#39;: 1.42498154722369,
 &#39;field&#39;: 0.9011905403991105,
 &#39;run&#39;: 0.7856686087514119,
 &#39;fiberid&#39;: 0.48419622387041755,
 &#39;dec&#39;: 0.40877000998928603,
 &#39;g&#39;: 0.3113040809109803,
 &#39;ra&#39;: 0.3112717293491661,
 &#39;r&#39;: 0.25950045766926433,
 &#39;camcol&#39;: 0.24487353532627085}
</pre></div>


<p>While XGBoost had the ability to use matplotlib to plot, Catboost's equivalent is brought to us by shap.</p>
<div class="highlight"><pre><span></span>explainer = shap.TreeExplainer(model2)
shap_values = explainer.shap_values(Pool(Z_train2, y_train, cat_features=categorical_features))
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/catboost/core.py:1697: UserWarning: &#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead
  warnings.warn(&quot;&#39;fstr_type&#39; parameter will be deprecated soon, use &#39;type&#39; parameter instead&quot;)
The model has complex ctrs, so the SHAP values will be calculated approximately.
</pre></div>


<div class="highlight"><pre><span></span>explainer.expected_value
</pre></div>


<p>[0.10654885020023455, 1.5818176696303439, -1.6883665198535525]</p>
<div class="highlight"><pre><span></span>shap.summary_plot(shap_values, Z_train2, plot_type=&quot;bar&quot;)
</pre></div>


<p><img alt="png" src="images/blog_3_48_0.png"></p>
<p>Above we've done a basic model, and we've discovered that redshift is the value that has the greatest effect on the decision trees. One of the best features about CatBoost is that we can now fine-tune parameters in order to speed up the model.</p>
<div class="highlight"><pre><span></span>fast_model = CatBoostClassifier(
    random_seed=62,
    iterations=150, #The default is 1000 trees. Lowering the number of trees can speed up the model. 
    learning_rate=0.01, #Typically, if you lower the number of trees, you should raise the learning rate.
    boosting_type=&#39;Plain&#39;, #The default is Ordered. This prevents overfitting but is expensive in computation.
    bootstrap_type=&#39;Bernoulli&#39;, #The default is Bayesian. Bernoulli is faster.
    subsample=0.5, #less than 1 is optimal for speed.
    rsm=0.5, #random subspace method speeds up the training
    leaf_estimation_iterations=5, #for smaller datasets this should be set at 1 or 5.
    max_ctr_complexity=1)
#We could also set one_hot_max_size to larger than the default 2. The default 2 means that any category that has 2 
#or less values will be hot encoded. The rest will be computated by catboosts algorithm. This is more computationally
#expensive.
fast_model.fit(
    Z_train2, y_train,
    cat_features=categorical_features,
    verbose=False,
    plot=False
)
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c1e615080&gt;
</pre></div>


<p><img src='images/newplot2.png' alt='newplot2'></p>
<p>In addition to the above parameters that were adjusted to speed up the model's performance, there are also more parameters that can be used to fine-tune the model.</p>
<div class="highlight"><pre><span></span>tuned_model = CatBoostClassifier(
    iterations=500,
    depth = 6, # how deep trees will go. Usually best between 1 and 10
    learning_rate=0.03,
    l2_leaf_reg=3, #regularizer value for Ridge regression
    bagging_temperature=1,
    one_hot_max_size=2, #2 is the default value. 
    leaf_estimation_method=&#39;Newton&#39;,
    loss_function=&#39;MultiClass&#39;
)
tuned_model.fit(
    Z_train2, y_train,
    cat_features=categorical_features,
    verbose=False,
    eval_set=(Z_val2, y_val),
    plot=False
)
</pre></div>


<div class="highlight"><pre><span></span>&lt;catboost.core.CatBoostClassifier at 0x1c400a10b8&gt;
</pre></div>


<p><img src='images/newplot3.png' alt='newplot3'></p>
<div class="highlight"><pre><span></span>final_model = CatBoostClassifier(
    random_seed=63,
    iterations=int(tuned_model.tree_count_),
)
final_model.fit(
    Z_train2, y_train,
    cat_features=categorical_features,
    verbose=100
)
</pre></div>


<div class="highlight"><pre><span></span>Learning rate set to 0.055965
0:  learn: 0.5431275    total: 61.7ms   remaining: 30.8s
100:    learn: 0.0057702    total: 6.14s    remaining: 24.3s
200:    learn: 0.0032198    total: 11.3s    remaining: 16.8s
300:    learn: 0.0021840    total: 16.3s    remaining: 10.8s
400:    learn: 0.0016674    total: 21.1s    remaining: 5.21s
499:    learn: 0.0013561    total: 25.5s    remaining: 0us





&lt;catboost.core.CatBoostClassifier at 0x1c1e2a59e8&gt;
</pre></div>


<div class="highlight"><pre><span></span>print(final_model.get_best_score())
</pre></div>


<div class="highlight"><pre><span></span>{&#39;learn&#39;: {&#39;Logloss&#39;: 0.0013560782767138484}}
</pre></div>


<p>The greatest benefit of CatBoost is its ability to handle categorical features. With other boosting software, we need to preprocess data on our own. This would be done using LabelBinarizer (as shown above in the XGBoost section). Although DataFrameMapper makes this task more manageable, in some cases it may lead to datasets that are unimaginably large. In addition to combination features(combining columns), which are converted to number values (these combination features are not used in the first split, but then all will be used in the next splits), CatBoost has a special formula to convert categorical features into numbers. With CatBoost and its categorical features, the data is shuffled and a mean is calculated for every object on its historical data.</p>
<p>ctr i = (countInClass + prior) / (totalCount + 1)</p>
<p>CountInClass is how many times the label value was equal to i for objects with the current categorical feature value.</p>
<p>TotalCount is the total number of objects already visited that have the same feature value as the one being visited.</p>
<p>Prior is a constant defined by the starting parameters</p>
<p>When it comes to missing values, CatBoost also deals with these during training. The default setting is 'Min' which means that missing values are processed as the minimum value for the feature. It is also guaranteed that a split that seperates all missing values from all other values is considered when selecting trees.</p>
<h1>Conclusion</h1>
<p>In addition to categorical features, another main diference between XGBoost and CatBoost is the type of decision trees used. XGBoost uses assymetric trees (which could be viewed above) while CatBoost uses oblivious trees as base predictors. An assymetric tree makes splits that will learn the most. Oblivious trees simply means that the feature used for splitting is the same accross all intermediate nodes within the same level of the tree.</p>
<p>Overall, I find it's hard to choose the best boosting system, because both do a great job and offer great features to work with. This can be viewed by their use in winning Kaggle competition entries. However, I find CatBoost's ability to pass categorical features, instead of one-hot-encoding, novel. It saves time in preparing the data to be modeled and although more computationally expensive at first, I think with larger datasets, where one-hot-encoding could swell the dataset to enormous sizes, it would be more effective. I like the fact that it provides both options when you're fitting a model. The default is 2 - which means that if a categorical column has two or less categories it will hot encode, and if its more than 2, it will run its built-in calculation for categorical columns. Either way, categorical columns are processed internally when training the model. Thus, even if you prefer categorical columns to be one-hot-encoded, you can set the parameters accordingly and not have to pre-process these columns. </p>
<p>It's hard to choose between the two libraries because I think they both excel with larger datasets than the one I used. However, with my computer and the SkyServer dataset, CatBoost outperformed XGBoost in speed, and I found the ability to fine-tune and speed-up the model more user-friendly and intuitive. Also I found the instant plotting more engaging, and the shap add-on, more visually appealing.</p>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_2.html" rel="bookmark">Don't be a Dummy, (For)get_dummies!</a></h2>
            <time datetime="2019-04-10T10:20:00-04:00">
                Apr 10, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p>pd.get_dummies is a pandas function that transforms a column containing categorical variables into a series of new columns composed of 1s and 0s (true and false). The significance in this process is that in order for any model or algorithm to process categorical features, they must first be transformed into numerical representation. Why not assign categorical features a number between 1 and 10 and keep them in one column? This process would result in assigning irrelevant values to items in a categorical column that would be misinterpreted by an algorithm. More columns composed of 1s and 0s is preferred over a single column, because it has the benefit of categories not weighting a value improperly.</p>
<p>To show how Get_Dummies works, I will create a dictionary of hockey players and transform it into a DataFrame:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="p">({</span>

    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Sidney Crosby&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;31&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Pittsburgh Penguins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Nikita Kucherov&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;25&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Tampa Bay Lightning&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;RW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Connor McDavid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;22&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Edmonton Oilers&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Mikko Rantanen&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;22&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s2">&quot;Colorado Avalanche&quot;</span><span class="p">,</span> 
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;RW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Finnish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alex Ovechkin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;33&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Washington Capitals&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;LW&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Braden Holtby&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;29&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Washington Capitals&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Brent Burns&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;34&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;San Jose Sharks&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;John Tavares&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;28&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Toronto Maple Leafs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span> 
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Mark Scheiele&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;26&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Winnipeg Jets&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Carey Price&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;31&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Montreal Canadians&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;G&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Morgan Rielly&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;25&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Toronto Maple Leafs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;American&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>

<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Nathan MacKinnon&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;23&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Colorado Avalanche&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Canadian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;no&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Evgeni Malkin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;32&#39;</span><span class="p">,</span>
    <span class="s1">&#39;team&#39;</span><span class="p">:</span> <span class="s1">&#39;Pittsburgh Penguins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;position&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nationality&#39;</span><span class="p">:</span> <span class="s1">&#39;Russian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hart&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stanley_cup&#39;</span><span class="p">:</span> <span class="s1">&#39;yes&#39;</span>
<span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hockey_players</span><span class="p">)</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;nationality&#39;</span><span class="p">,</span> <span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">,</span> <span class="s1">&#39;stanley_cup&#39;</span><span class="p">,</span> <span class="s1">&#39;hart&#39;</span><span class="p">]]</span>
<span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">HTML</span><span class="p">(</span><span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-stripped table-hover&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-stripped table-hover">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>age</th>
      <th>nationality</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sidney Crosby</td>
      <td>31</td>
      <td>Canadian</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Nikita Kucherov</td>
      <td>25</td>
      <td>Russian</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Connor McDavid</td>
      <td>22</td>
      <td>Canadian</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mikko Rantanen</td>
      <td>22</td>
      <td>Finnish</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Alex Ovechkin</td>
      <td>33</td>
      <td>Russian</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Above is a simple dataset consisiting of 13 National Hockey League players. For each player, we are provided with</p>
<div class="highlight"><pre><span></span>    -name 
    -age 
    -nationality 
    -team 
    -position 
    -whether he has won a Stanley Cup 
    -whether he has won the Hart trophy.
</pre></div>


<p>For the purpose of this blog, let's imagine that this is a much larger dataset consisting of all NHL players. As I mentioned above, in order to compare categorical variables, we need to transform them into 1s and 0s. </p>
<p>If a column is categorical and based on two inputs, forexample 'yes' and 'no', this can be as simple as: </p>
<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;stanley_cup&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;stanley_cup&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;yes&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span></span>                            OR
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>age</th>
      <th>nationality</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sidney Crosby</td>
      <td>31</td>
      <td>Canadian</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Nikita Kucherov</td>
      <td>25</td>
      <td>Russian</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Connor McDavid</td>
      <td>22</td>
      <td>Canadian</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mikko Rantanen</td>
      <td>22</td>
      <td>Finnish</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Alex Ovechkin</td>
      <td>33</td>
      <td>Russian</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>Whenever the column is more complex, this is when someone would reach for pd.get_dummies.</p>
<p>Say we thought that the nationality of a player was indicative of performance. In order to use this categorical value in a model, we would need to transform these values into 1s and 0s. </p>
<p>Now I am going to show how to use pd.get_dummies to transform the nationality column into multiple columns that correspond with the nationality of each player. </p>
<div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;nationality&#39;</span><span class="p">])</span>
<span class="n">dummy</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>The function provided us with 4 columns each representing 1 of 4 nationalities in the dataset. In order to compare this information to the original dataset, we will concatenate onto the original DataFrame. While we are doing that, we will drop the original column for nationality, as well as the name category, as each player is already identfied by an id number.</p>
<div class="highlight"><pre><span></span><span class="n">hockey_players</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hockey_players</span><span class="p">,</span> <span class="n">dummy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;nationality&#39;</span><span class="p">])</span>
<span class="n">hockey_players</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">])</span>
<span class="n">hockey_players</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>hart</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>At first sight, this seems like a very useful tool provided by pandas. We can now compare the nationality of each player to the values in the other columns.</p>
<p>However, there is one very large downside to using pd.get_dummies, which I will demonstrate below.</p>
<p>Imagine that we were attempting to see whether all these factors could be used to make a model that predicted whether a player would win the Hart trophy. First we will identify our features and our target.</p>
<div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">cols</span> <span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">hockey_players</span> <span class="k">if</span> <span class="n">cols</span> <span class="o">!=</span> <span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span>
</pre></div>


<p>Since the dataset is clean (no missing values), the next thing we will do is split the data into training and testing sets.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<p>Now we have two sets of data:</p>
<div class="highlight"><pre><span></span>-One to train our model on (X_train)
-One to test our final model on (X_test)
</pre></div>


<p>Next we look at our X_train and we see that we will need to use pd.get_dummies on the categorical columns for position and team. </p>
<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>team_Montreal Canadians</th>
      <th>team_Pittsburgh Penguins</th>
      <th>team_Tampa Bay Lightning</th>
      <th>team_Toronto Maple Leafs</th>
      <th>team_Washington Capitals</th>
      <th>position_D</th>
      <th>position_G</th>
      <th>position_LW</th>
      <th>position_RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>29</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Next, we will do the same thing for the testing data.</p>
<div class="highlight"><pre><span></span><span class="n">X_test_with_dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>Now with our training and testing data hot-encoded, we are ready to model! Let's instantiate the Logistic Regression and fit and score our model.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_with_dummy</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_with_dummy</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">linear_model</span><span class="o">/</span><span class="n">logistic</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">433</span><span class="p">:</span> <span class="ne">FutureWarning</span><span class="p">:</span> <span class="n">Default</span> <span class="n">solver</span> <span class="n">will</span> <span class="n">be</span> <span class="n">changed</span> <span class="n">to</span> <span class="s1">&#39;lbfgs&#39;</span> <span class="ow">in</span> <span class="mf">0.22</span><span class="o">.</span> <span class="n">Specify</span> <span class="n">a</span> <span class="n">solver</span> <span class="n">to</span> <span class="n">silence</span> <span class="n">this</span> <span class="n">warning</span><span class="o">.</span>
  <span class="ne">FutureWarning</span><span class="p">)</span>



<span class="o">---------------------------------------------------------------------------</span>

<span class="ne">ValueError</span>                                <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">13</span><span class="o">-</span><span class="mi">7057</span><span class="n">dac4b7c1</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
      <span class="mi">4</span> 
      <span class="mi">5</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_with_dummy</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="o">----&gt;</span> <span class="mi">6</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_with_dummy</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>


<span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">base</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="n">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="mi">286</span>         <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    287         from .metrics import accuracy_score</span>
<span class="s2">--&gt; 288         return accuracy_score(y, self.predict(X), sample_weight=sample_weight)</span>
<span class="s2">    289 </span>
<span class="s2">    290</span>


<span class="s2">/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py in predict(self, X)</span>
<span class="s2">    279             Predicted class label per sample.</span>
<span class="s2">    280         &quot;&quot;&quot;</span>
<span class="o">--&gt;</span> <span class="mi">281</span>         <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="mi">282</span>         <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="mi">283</span>             <span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">6</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">linear_model</span><span class="o">/</span><span class="n">base</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="n">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="mi">260</span>         <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
    <span class="mi">261</span>             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X has </span><span class="si">%d</span><span class="s2"> features per sample; expecting </span><span class="si">%d</span><span class="s2">&quot;</span>
<span class="o">--&gt;</span> <span class="mi">262</span>                              <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_features</span><span class="p">))</span>
    <span class="mi">263</span> 
    <span class="mi">264</span>         <span class="n">scores</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>


<span class="ne">ValueError</span><span class="p">:</span> <span class="n">X</span> <span class="n">has</span> <span class="mi">9</span> <span class="n">features</span> <span class="n">per</span> <span class="n">sample</span><span class="p">;</span> <span class="n">expecting</span> <span class="mi">15</span>
</pre></div>


<p>VALUE ERROR! </p>
<p>When we used pd.get_dummies to make dummy columns that were hot-encoded, we over-looked the fact that the testing data was much smaller and it did not have examples for each categorical variable. Although we performed pd.get_dummies on the testing data, as we did on the training data, there was no way for the transformed testing data to know that it was missing certain columns that were present in the original data set and still existed in the training data.</p>
<p>Observe the shapes of our two data sets:</p>
<div class="highlight"><pre><span></span><span class="n">X_train_with_dummy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(10, 15)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_test_with_dummy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(3, 9)
</pre></div>


<p>Some may argue that this could easily be avoided by using pd.get_dummies before splitting the data into training and testing sets. However, every good Data Scientist knows the importance of using train_test_split first before tampering with your dataset.</p>
<p>These two ideas seem at odds, because they are! What to do?</p>
<p>Luckily SkLearn provides us with a better and more advanced function which considers this possible dilemna. Because of this, SkLearn's Label Binarizer should be used instead of pd.get_dummies.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">cols</span> <span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">hockey_players</span> <span class="k">if</span> <span class="n">cols</span> <span class="o">!=</span> <span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">hockey_players</span><span class="p">[</span><span class="s1">&#39;hart&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">])</span>
<span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>      <span class="c1">#This is where the magic happens!</span>
<span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">])</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_train</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>29</td>
      <td>Washington Capitals</td>
      <td>G</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>34</td>
      <td>San Jose Sharks</td>
      <td>D</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>0</th>
      <td>31</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>Tampa Bay Lightning</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33</td>
      <td>Washington Capitals</td>
      <td>LW</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>The built in method call 'classes_' is the saviour in this function. By fitting the training data and than labeling the columns based on the method call 'classes_', the existence of all 5 positions is remembered. Simply put, the attribute 'classes_' holds the label for each class. Now we will transform our test data.</p>
<div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;position&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_test</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>28</td>
      <td>Toronto Maple Leafs</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>26</td>
      <td>Winnipeg Jets</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>31</td>
      <td>Montreal Canadians</td>
      <td>G</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>As you can see, even though none of the players in the testing data played wing, instantiating Label Binarizer and utilizing the method 'classes_' ensures that these columns were preserved in the testing data.</p>
<p>Now let's perform LabelBinarizer on the team column as well!</p>
<div class="highlight"><pre><span></span><span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">])</span>
<span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>     
<span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">])</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>team</th>
      <th>position</th>
      <th>stanley_cup</th>
      <th>American</th>
      <th>Canadian</th>
      <th>Finnish</th>
      <th>Russian</th>
      <th>C</th>
      <th>D</th>
      <th>G</th>
      <th>LW</th>
      <th>RW</th>
      <th>Colorado Avalanche</th>
      <th>Edmonton Oilers</th>
      <th>Pittsburgh Penguins</th>
      <th>San Jose Sharks</th>
      <th>Tampa Bay Lightning</th>
      <th>Toronto Maple Leafs</th>
      <th>Washington Capitals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>32</td>
      <td>Pittsburgh Penguins</td>
      <td>C</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>25</td>
      <td>Toronto Maple Leafs</td>
      <td>D</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>22</td>
      <td>Colorado Avalanche</td>
      <td>RW</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>23</td>
      <td>Colorado Avalanche</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Edmonton Oilers</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">]),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>    <span class="c1">#Remembering the classes created from the original categorical column.</span>
    <span class="n">index</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">])</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;team&#39;</span><span class="p">,</span> <span class="s1">&#39;position&#39;</span><span class="p">])</span>
</pre></div>


<p>Now let's see if we can model our data...</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)





0.6666666666666666
</pre></div>


<p>SUCCESS THEY'RE THE SAME SIZE!!!</p>
<p>LabelBinarizer is better than pd.get_dummies because when you fit your training data it gives you back a class_ method which remembers the different categories that were created when you hot encoded your column into multiple columns. When you transform your testing data and call the class attributes that were stored when you fit your training data, even though your test data may not have samples from that specific column, it remembers that it needs to represent the column even though there are no 1s present.</p>
<div class="highlight"><pre><span></span>
</pre></div>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="https://KristiGourlay.github.io/blog/blog_1.html" rel="bookmark">The Functionality of Functions</a></h2>
            <time datetime="2019-03-28T10:20:00-04:00">
                Mar 28, 2019
            </time>
        </header>
    </div>

    <div class="span2">
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <div class="article-summary">
        <!-- show full content instead of summary -->
        <p><em>The Functionality of Functions</em></p>
<p>Coming from an academic background based in History, Philosophy, and Law, learning Data Science can be  overwhelming; especially when you haven't taken a Computer or Math course in TWELVE years. The biggest issues I have had in the first three weeks deal with syntax. Learning Python is the equivalent of learning a new language. The one area I have continued to struggle with is how to develop new functions. While this is by far not the hardest task thrown at us, it is essential to most tasks a Data Scientist faces daily.</p>
<p>To start, writing a basic function is very simple.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_my_name</span><span class="p">(</span><span class="n">first_name</span><span class="p">,</span> <span class="n">last_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">last_name</span><span class="p">,</span> <span class="n">first_name</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">print_my_name</span><span class="p">(</span><span class="s1">&#39;kristi&#39;</span><span class="p">,</span> <span class="s1">&#39;gourlay&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(&#39;gourlay&#39;, &#39;kristi&#39;)
</pre></div>


<p>OR</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adder</span><span class="p">(</span><span class="n">number1</span><span class="p">,</span> <span class="n">number2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">number1</span> <span class="o">+</span> <span class="n">number2</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">adder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>5
</pre></div>


<p>They get slightly more difficult when you're asked to count something or create a list. </p>
<p>Consider the following function that counts the vowels in a provided word.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vowel_counter</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vowels</span> <span class="o">=</span> <span class="s1">&#39;aeiou&#39;</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">vowels</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">return</span> <span class="n">counter</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">vowel_counter</span><span class="p">(</span><span class="s1">&#39;elephant&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>1
</pre></div>


<p>WAIT! That's not right!</p>
<p>Lesson One: Make sure the return is outside the loop!
This placement of the return call had me pulling my hair out for the entire first week. I could not understand why my functions, that looked just like my classmates' functions, were not working. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vowel_counter</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vowels</span> <span class="o">=</span> <span class="s1">&#39;aeiou&#39;</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">vowels</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">counter</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">vowel_counter</span><span class="p">(</span><span class="s1">&#39;elephant&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>3
</pre></div>


<p>While writing a simple function is easy, the next step is learning to write a function that can be used on data or a dictionary. I struggled with this in our first lab.</p>
<p>Below I have created a dictionary of tv shows:</p>
<div class="highlight"><pre><span></span><span class="n">tvshows</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;curb your enthusiasm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the wire&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;shameless&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;dramedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;showtime&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the sopranos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;game of thrones&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;hbo&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;house of cards&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;netflix&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;kimmy schmidt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;netflix&#39;</span>
<span class="p">}]</span>

<span class="k">print</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[{&#39;name&#39;: &#39;curb your enthusiasm&#39;, &#39;category&#39;: &#39;comedy&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;the wire&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;shameless&#39;, &#39;category&#39;: &#39;dramedy&#39;, &#39;network&#39;: &#39;showtime&#39;}, {&#39;name&#39;: &#39;the sopranos&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;game of thrones&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;hbo&#39;}, {&#39;name&#39;: &#39;house of cards&#39;, &#39;category&#39;: &#39;drama&#39;, &#39;network&#39;: &#39;netflix&#39;}, {&#39;name&#39;: &#39;kimmy schmidt&#39;, &#39;category&#39;: &#39;comedy&#39;, &#39;network&#39;: &#39;netflix&#39;}]
</pre></div>


<p>For example, I want to make a function to iterate through the dictionary and return all the shows that are from a specific network.</p>
<p>The first step is to create a 'for loop' OR use a 'list comprehension' to write what you want to get back. </p>
<p>In this instance, I would like to see all the tv show names that are on the HBO network.</p>
<p>Name a function and pass it two arguments. 
1)what you are going to iterate through.
2)what you are looking for.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tv_net</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="s1">&#39;tvshows&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;hbo&#39;</span><span class="p">):</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">tv</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">tv</span> <span class="ow">in</span> <span class="n">dictionary</span> <span class="k">if</span> <span class="n">tv</span><span class="p">[</span><span class="s1">&#39;network&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">network</span><span class="p">]</span>

<span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;curb your enthusiasm&#39;, &#39;the wire&#39;, &#39;the sopranos&#39;, &#39;game of thrones&#39;]
</pre></div>


<p>Since I set the second argument default to hbo, the function automatically looks for shows from the HBO network. Setting a default argument makes life easier, and helps you remember what type of argument you will be looking for. If I wanted to change the network argument, it's as simple as passing a different network through.</p>
<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;netflix&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;house of cards&#39;, &#39;kimmy schmidt&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">tvshows</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;showtime&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;shameless&#39;]
</pre></div>


<p>The important thing I learned from this process was the fact that you can then take the function you made and use it on similar data later in your code. For example, say I was then provided with a dictionary of prime time tv shows.</p>
<div class="highlight"><pre><span></span><span class="n">primetime</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;this is us&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;drama&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;family guy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;fox&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the good place&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;the office&#39;</span><span class="p">,</span>
    <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;network&#39;</span><span class="p">:</span> <span class="s1">&#39;nbc&#39;</span>
<span class="p">}</span>
<span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">primetime</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="s1">&#39;nbc&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;this is us&#39;, &#39;the good place&#39;, &#39;the office&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tv_net</span><span class="p">(</span><span class="n">primetime</span><span class="p">,</span> <span class="s1">&#39;fox&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;family guy&#39;]
</pre></div>


<p>This importance of using functions to iterate through data and dictionaries resurfaces in EDA. If we are looking at a set of data and need to change something about it, and we may need to make similar changes later in our code, it's practical to create a function.
To show this, I will use the tvshow sets from above. (First I will need to place them in a Data Frame)</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">tvshows</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
<span class="n">primetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">primetime</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tvshows</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category</th>
      <th>name</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>comedy</td>
      <td>curb your enthusiasm</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>1</th>
      <td>drama</td>
      <td>the wire</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>2</th>
      <td>dramedy</td>
      <td>shameless</td>
      <td>showtime</td>
    </tr>
    <tr>
      <th>3</th>
      <td>drama</td>
      <td>the sopranos</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>drama</td>
      <td>game of thrones</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>5</th>
      <td>drama</td>
      <td>house of cards</td>
      <td>netflix</td>
    </tr>
    <tr>
      <th>6</th>
      <td>comedy</td>
      <td>kimmy schmidt</td>
      <td>netflix</td>
    </tr>
  </tbody>
</table>
</div>

<p>When we look at the dataframe for tvshows, we notice that the category and the name columns need to be swapped. We could run a simple list command, but since we know we have other similar data, it may be easier to create a function so that we don't have to repeat these verbose commands later.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">swap_col</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cols</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">),</span> <span class="n">cols</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
    <span class="n">cols</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
    <span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dataframe</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">swap_col</span><span class="p">(</span><span class="n">tvshows</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>category</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>curb your enthusiasm</td>
      <td>comedy</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>1</th>
      <td>the wire</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>2</th>
      <td>shameless</td>
      <td>dramedy</td>
      <td>showtime</td>
    </tr>
    <tr>
      <th>3</th>
      <td>the sopranos</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>game of thrones</td>
      <td>drama</td>
      <td>hbo</td>
    </tr>
    <tr>
      <th>5</th>
      <td>house of cards</td>
      <td>drama</td>
      <td>netflix</td>
    </tr>
    <tr>
      <th>6</th>
      <td>kimmy schmidt</td>
      <td>comedy</td>
      <td>netflix</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now we import our other dataframe and we see the same item that needs to be fixed. </p>
<div class="highlight"><pre><span></span><span class="n">primetime</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category</th>
      <th>name</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>drama</td>
      <td>this is us</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>1</th>
      <td>comedy</td>
      <td>family guy</td>
      <td>fox</td>
    </tr>
    <tr>
      <th>2</th>
      <td>comedy</td>
      <td>the good place</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>3</th>
      <td>comedy</td>
      <td>the office</td>
      <td>nbc</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">swap_col</span><span class="p">(</span><span class="n">primetime</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>category</th>
      <th>network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this is us</td>
      <td>drama</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>1</th>
      <td>family guy</td>
      <td>comedy</td>
      <td>fox</td>
    </tr>
    <tr>
      <th>2</th>
      <td>the good place</td>
      <td>comedy</td>
      <td>nbc</td>
    </tr>
    <tr>
      <th>3</th>
      <td>the office</td>
      <td>comedy</td>
      <td>nbc</td>
    </tr>
  </tbody>
</table>
</div>

<p>The ability to make a function to fix mistakes in your data becomes a useful skill to have. It will be a time saver, and that's one of the reasons why learning functions is quite important for Data Science. </p>
<p>One last point.</p>
<p>I've shown how functions become important when both analyzing and fixing your data. Sometimes you might need your function to include multiple steps and the sheer volume may seem daunting. The number one tip, I have taken away in my first three weeks learning Python is to break things down. If you take everything step by step and work your way up to the return you need, the task becomes much easier.</p>
<p>It's easier to do many smaller pieces of the whole, than to tackle the whole.</p>
<p>STEP ONE: DONT GET OVERWHELMED!</p>
<p>STEP TWO: SIMPLY WRITE DOWN WHAT THE FUNCTION NEEDS TO DO/RETURN</p>
<p>STEP THREE: WRITE DOWN IN ORDER WHAT YOU NEED TO DO TO GET THE DESIRED RETURN</p>
<p>The perfect example of this breaking down a function was presented to me in our first project. We were asked to analyze SAT and ACT data. At one point in the assignment, we were asked to design a function that returns the standard deviation for a column. This question took me 10x longer than any other question, because I kept thinking about the final product. Eventually, I realised I needed to follow the three steps laid out above.</p>
<p>After calming myself down, I answered STEP TWO. The function needs to return the standard deviation of a column in the SAT/ACT dataset. </p>
<p>STEP THREE: How do you calculate standard deviation?</p>
<ol>
<li>
<p>Calculate the mean</p>
<p>mean = sum(column) / len(column)</p>
</li>
<li>
<p>For each number, subtract the mean and square the results</p>
<div class="highlight"><pre><span></span>((n - (sum(column) / len(column)))**2)
</pre></div>


</li>
<li>
<p>Find the mean of those squared differences( START A LIST before the math. new_list = [] )</p>
<p>new_list.append(((n - (sum(column) / len(column)))**2))</p>
</li>
<li>
<p>Take the square root of that list</p>
<p>math.sqrt(((sum(new_list))) / (len(column) - 1))</p>
</li>
</ol>
<p>Start with the first thing you need to do and slowly add what you need next. The result is:   </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">calc_std</span><span class="p">(</span><span class="n">column</span><span class="p">):</span>
    <span class="n">new_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">column</span><span class="p">:</span>
        <span class="n">new_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">column</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">column</span><span class="p">)))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="nb">sum</span><span class="p">(</span><span class="n">new_list</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">column</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">column</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">calc_std</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>2.416461403433896
</pre></div>


<p>Writing functions was something that I alone seemed to struggle with in my cohort. However, my focus on improving my ability to write functions, allowed me to highlight other areas in my work that I should keep an eye on. If you're code is not working, read the return for details, if you still cannot figure out what's wrong, double check these few things:
* Placement
* Accuracy
* Brackets!</p>
<p>Placement: Make sure your return is located outside the loop. But also make sure that lists and counters are located in the right place as well. If needed for the function, place the lists right below the function call.</p>
<p>Accuracy: The amount of times my code was not working because I mis-spelt a word in one place. Spelling will always be important, not just in academia!</p>
<p>Brackets: If in doubt look if you're missing a bracket somewhere. I spent an hour with a function not working, to realize I just needed to square bracket the argument I was submitting.</p>
<p>Attention to detail!</p>
<div class="highlight"><pre><span></span>
</pre></div>
    </div>
 
            </article>
        </section>
    </div> <!-- span -->
</div>
<div class="span7 offset2">
    <p class="paginator">
        Page 1 / 1
    </p>
</div>
    </div>
                        </div>
                    </div> <!-- content row -->

                    <div class="row"> <!-- footer -->
                        <div class="span7 offset2">
                            <footer id="site-footer">
                                <p>
                                    <a href="" target="_blank">
                                        
                                    </a> Kristi Gourlay /
                                    <a href="https://KristiGourlay.github.io/blog/">Archives</a>
                                </p>
                                <p>
                                    Powered by
                                    <a href="https://github.com/getpelican/pelican" target="_blank">
                                        Pelican
                                    </a>
                                    /
                                    Source code on
                                    <a href="http://github.com/siovene/iovene.com" target="blank">
                                        GitHub
                                    </a>
                                    /
                                    Theme <code>Lannisport</code>, also on
                                    <a href="http://github.com/siovene/lannisport" target="_blank">
                                        GitHub
                                    </a>
                                </p>
                            </footer>
                        </div> <!-- footer span -->
                    </div> <!-- footer row -->
                </div> <!-- header+content+footer span -->

                <div class="span3">
                    <aside class="affix">
                        <div id="sidebar">
                            <div id="social-icons">
                            </div>

                            <div id="navigation">
<nav>
	<h2>Pages</h2>
	<ul>
		<li>
			<ul>
				<li>
					<a href="https://KristiGourlay.github.io/blog">Blog</a>
				</li>

			</ul>
		</li>
	</ul>
</nav>

<nav>
	<h2>Categories</h2>
	<ul>
		<li>
			<ul>
					<li>
						<a href="https://KristiGourlay.github.io/blog/category/misc.html">misc</a>
					</li>
			</ul>
		</li>
	</ul>
</nav>                            </div>
                        </div> <!-- sidebar -->

                        <p id="back-to-top">
                            <a href="#">
                                <i class="icon-double-angle-up"></i> Back to top
                            </a>
                        </p>

                    </aside>
                </div> <!-- logo+navigation span -->
            </div> <!-- row -->

        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>

        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/js/bootstrap.min.js"></script>
        <script>$('body').popover || document.write('<script src="https://KristiGourlay.github.io/blog/theme/js/vendor/bootstrap.min.js"><\/script>')</script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/jquery.captions.js"></script>
        <script src="https://KristiGourlay.github.io/blog/theme/js/vendor/jquery.colorbox-min.js"></script>

        <script src="https://KristiGourlay.github.io/blog/theme/js/main.js"></script>

        <script>
            var _gaq=[['_setAccount',''],['_trackPageview'],['_trackPageLoadTime'],['_gat._anonymizeIp']];
            (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
            g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
            s.parentNode.insertBefore(g,s)}(document,'script'));
        </script>
    </body>
</html>